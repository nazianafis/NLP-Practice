{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's set some text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"If you can talk with crowds and keep your virtue,\\nOr walk with Kings—nor lose the common touch.\\nIf neither foes nor loving friends can hurt you,\\nIf all men count with you, but none too much.\\nIf you can fill the unforgiving minute,\\nWith sixty seconds’ worth of distance run,\\nYours is the Earth and everything that’s in it,\\nAnd—which is more—you’ll be a Man, my son!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to split the text based on spaces (default function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'talk',\n",
       " 'with',\n",
       " 'crowds',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'your',\n",
       " 'virtue,',\n",
       " 'Or',\n",
       " 'walk',\n",
       " 'with',\n",
       " 'Kings—nor',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'common',\n",
       " 'touch.',\n",
       " 'If',\n",
       " 'neither',\n",
       " 'foes',\n",
       " 'nor',\n",
       " 'loving',\n",
       " 'friends',\n",
       " 'can',\n",
       " 'hurt',\n",
       " 'you,',\n",
       " 'If',\n",
       " 'all',\n",
       " 'men',\n",
       " 'count',\n",
       " 'with',\n",
       " 'you,',\n",
       " 'but',\n",
       " 'none',\n",
       " 'too',\n",
       " 'much.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'fill',\n",
       " 'the',\n",
       " 'unforgiving',\n",
       " 'minute,',\n",
       " 'With',\n",
       " 'sixty',\n",
       " 'seconds’',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'distance',\n",
       " 'run,',\n",
       " 'Yours',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Earth',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'that’s',\n",
       " 'in',\n",
       " 'it,',\n",
       " 'And—which',\n",
       " 'is',\n",
       " 'more—you’ll',\n",
       " 'be',\n",
       " 'a',\n",
       " 'Man,',\n",
       " 'my',\n",
       " 'son!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that some words are not well separated from punctuation.\n",
    "So let's try to remove those characters... but before that, let's create a quick feature vector first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And—which',\n",
       " 'Earth',\n",
       " 'If',\n",
       " 'Kings—nor',\n",
       " 'Man,',\n",
       " 'Or',\n",
       " 'With',\n",
       " 'Yours',\n",
       " 'a',\n",
       " 'all',\n",
       " 'and',\n",
       " 'be',\n",
       " 'but',\n",
       " 'can',\n",
       " 'common',\n",
       " 'count',\n",
       " 'crowds',\n",
       " 'distance',\n",
       " 'everything',\n",
       " 'fill',\n",
       " 'foes',\n",
       " 'friends',\n",
       " 'hurt',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it,',\n",
       " 'keep',\n",
       " 'lose',\n",
       " 'loving',\n",
       " 'men',\n",
       " 'minute,',\n",
       " 'more—you’ll',\n",
       " 'much.',\n",
       " 'my',\n",
       " 'neither',\n",
       " 'none',\n",
       " 'nor',\n",
       " 'of',\n",
       " 'run,',\n",
       " 'seconds’',\n",
       " 'sixty',\n",
       " 'son!',\n",
       " 'talk',\n",
       " 'that’s',\n",
       " 'the',\n",
       " 'too',\n",
       " 'touch.',\n",
       " 'unforgiving',\n",
       " 'virtue,',\n",
       " 'walk',\n",
       " 'with',\n",
       " 'worth',\n",
       " 'you',\n",
       " 'you,',\n",
       " 'your']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sorted(sentence.split()) # splitting based on spaces\n",
    "vocab = sorted(set(tokens)) # sorting and removing duplicates by using set()\n",
    "vocab # just printing the vocab so we can look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the order is: numbers first, followerd by capital letters, and then lower case letters (all alphabetically sorted). We also note that some repeating words appear only once in the vocabulary list. Let's compare the size of the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 68\n",
      "vocab: 55\n"
     ]
    }
   ],
   "source": [
    "tokens_len = len(tokens)\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "print(\"tokens:\", tokens_len)\n",
    "print(\"vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and print the matrix of tokens against vocabulary. We will use the numpy lib for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.zeros((tokens_len, vocab_len), int)\n",
    "for i, token in enumerate(tokens):\n",
    "    matrix[i, vocab.index(token)] = 1\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more readable, we can use Pandas and DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>And—which</th>\n",
       "      <th>Earth</th>\n",
       "      <th>If</th>\n",
       "      <th>Kings—nor</th>\n",
       "      <th>Man,</th>\n",
       "      <th>Or</th>\n",
       "      <th>With</th>\n",
       "      <th>Yours</th>\n",
       "      <th>a</th>\n",
       "      <th>all</th>\n",
       "      <th>...</th>\n",
       "      <th>too</th>\n",
       "      <th>touch.</th>\n",
       "      <th>unforgiving</th>\n",
       "      <th>virtue,</th>\n",
       "      <th>walk</th>\n",
       "      <th>with</th>\n",
       "      <th>worth</th>\n",
       "      <th>you</th>\n",
       "      <th>you,</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>And—which</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Earth</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you,</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you,</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           And—which  Earth  If  Kings—nor  Man,  Or  With  Yours  a  all  \\\n",
       "And—which          1      0   0          0     0   0     0      0  0    0   \n",
       "Earth              0      1   0          0     0   0     0      0  0    0   \n",
       "If                 0      0   1          0     0   0     0      0  0    0   \n",
       "If                 0      0   1          0     0   0     0      0  0    0   \n",
       "If                 0      0   1          0     0   0     0      0  0    0   \n",
       "...              ...    ...  ..        ...   ...  ..   ...    ... ..  ...   \n",
       "you                0      0   0          0     0   0     0      0  0    0   \n",
       "you                0      0   0          0     0   0     0      0  0    0   \n",
       "you,               0      0   0          0     0   0     0      0  0    0   \n",
       "you,               0      0   0          0     0   0     0      0  0    0   \n",
       "your               0      0   0          0     0   0     0      0  0    0   \n",
       "\n",
       "           ...  too  touch.  unforgiving  virtue,  walk  with  worth  you  \\\n",
       "And—which  ...    0       0            0        0     0     0      0    0   \n",
       "Earth      ...    0       0            0        0     0     0      0    0   \n",
       "If         ...    0       0            0        0     0     0      0    0   \n",
       "If         ...    0       0            0        0     0     0      0    0   \n",
       "If         ...    0       0            0        0     0     0      0    0   \n",
       "...        ...  ...     ...          ...      ...   ...   ...    ...  ...   \n",
       "you        ...    0       0            0        0     0     0      0    1   \n",
       "you        ...    0       0            0        0     0     0      0    1   \n",
       "you,       ...    0       0            0        0     0     0      0    0   \n",
       "you,       ...    0       0            0        0     0     0      0    0   \n",
       "your       ...    0       0            0        0     0     0      0    0   \n",
       "\n",
       "           you,  your  \n",
       "And—which     0     0  \n",
       "Earth         0     0  \n",
       "If            0     0  \n",
       "If            0     0  \n",
       "If            0     0  \n",
       "...         ...   ...  \n",
       "you           0     0  \n",
       "you           0     0  \n",
       "you,          1     0  \n",
       "you,          1     0  \n",
       "your          0     1  \n",
       "\n",
       "[68 rows x 55 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(matrix, columns=vocab, index=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot clearer.\n",
    "\n",
    "Let's now build the bag of words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And—which', 1),\n",
       " ('Earth', 1),\n",
       " ('If', 1),\n",
       " ('Kings—nor', 1),\n",
       " ('Man,', 1),\n",
       " ('Or', 1),\n",
       " ('With', 1),\n",
       " ('Yours', 1),\n",
       " ('a', 1),\n",
       " ('all', 1),\n",
       " ('and', 1),\n",
       " ('be', 1),\n",
       " ('but', 1),\n",
       " ('can', 1),\n",
       " ('common', 1),\n",
       " ('count', 1),\n",
       " ('crowds', 1),\n",
       " ('distance', 1),\n",
       " ('everything', 1),\n",
       " ('fill', 1),\n",
       " ('foes', 1),\n",
       " ('friends', 1),\n",
       " ('hurt', 1),\n",
       " ('in', 1),\n",
       " ('is', 1),\n",
       " ('it,', 1),\n",
       " ('keep', 1),\n",
       " ('lose', 1),\n",
       " ('loving', 1),\n",
       " ('men', 1),\n",
       " ('minute,', 1),\n",
       " ('more—you’ll', 1),\n",
       " ('much.', 1),\n",
       " ('my', 1),\n",
       " ('neither', 1),\n",
       " ('none', 1),\n",
       " ('nor', 1),\n",
       " ('of', 1),\n",
       " ('run,', 1),\n",
       " ('seconds’', 1),\n",
       " ('sixty', 1),\n",
       " ('son!', 1),\n",
       " ('talk', 1),\n",
       " ('that’s', 1),\n",
       " ('the', 1),\n",
       " ('too', 1),\n",
       " ('touch.', 1),\n",
       " ('unforgiving', 1),\n",
       " ('virtue,', 1),\n",
       " ('walk', 1),\n",
       " ('with', 1),\n",
       " ('worth', 1),\n",
       " ('you', 1),\n",
       " ('you,', 1),\n",
       " ('your', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = {} # setting this up as a dictionary\n",
    "\n",
    "for token in tokens:\n",
    "    bow[token] = 1\n",
    "\n",
    "sorted(bow.items()) # lets print it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since bow is a dictionary, we see that the same words will not duplicate.\n",
    "\n",
    "Pandas also has a more efficient form of a dictionary called Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>And—which</th>\n",
       "      <th>Earth</th>\n",
       "      <th>If</th>\n",
       "      <th>Kings—nor</th>\n",
       "      <th>Man,</th>\n",
       "      <th>Or</th>\n",
       "      <th>With</th>\n",
       "      <th>Yours</th>\n",
       "      <th>a</th>\n",
       "      <th>all</th>\n",
       "      <th>...</th>\n",
       "      <th>too</th>\n",
       "      <th>touch.</th>\n",
       "      <th>unforgiving</th>\n",
       "      <th>virtue,</th>\n",
       "      <th>walk</th>\n",
       "      <th>with</th>\n",
       "      <th>worth</th>\n",
       "      <th>you</th>\n",
       "      <th>you,</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      And—which  Earth  If  Kings—nor  Man,  Or  With  Yours  a  all  ...  \\\n",
       "sent          1      1   1          1     1   1     1      1  1    1  ...   \n",
       "\n",
       "      too  touch.  unforgiving  virtue,  walk  with  worth  you  you,  your  \n",
       "sent    1       1            1        1     1     1      1    1     1     1  \n",
       "\n",
       "[1 rows x 55 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in tokens])), columns=['sent']).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>If</th>\n",
       "      <th>you</th>\n",
       "      <th>can</th>\n",
       "      <th>talk</th>\n",
       "      <th>with</th>\n",
       "      <th>crowds</th>\n",
       "      <th>and</th>\n",
       "      <th>keep</th>\n",
       "      <th>your</th>\n",
       "      <th>virtue,</th>\n",
       "      <th>...</th>\n",
       "      <th>that’s</th>\n",
       "      <th>in</th>\n",
       "      <th>it,</th>\n",
       "      <th>And—which</th>\n",
       "      <th>more—you’ll</th>\n",
       "      <th>be</th>\n",
       "      <th>a</th>\n",
       "      <th>Man,</th>\n",
       "      <th>my</th>\n",
       "      <th>son!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       If  you  can  talk  with  crowds  and  keep  your  virtue,  ...  \\\n",
       "sent0   1    1    1     1     1       1    1     1     1        1  ...   \n",
       "sent1   0    0    0     0     1       0    0     0     0        0  ...   \n",
       "sent2   1    0    1     0     0       0    0     0     0        0  ...   \n",
       "sent3   1    0    0     0     1       0    0     0     0        0  ...   \n",
       "sent4   1    1    1     0     0       0    0     0     0        0  ...   \n",
       "sent5   0    0    0     0     0       0    0     0     0        0  ...   \n",
       "sent6   0    0    0     0     0       0    1     0     0        0  ...   \n",
       "sent7   0    0    0     0     0       0    0     0     0        0  ...   \n",
       "\n",
       "       that’s  in  it,  And—which  more—you’ll  be  a  Man,  my  son!  \n",
       "sent0       0   0    0          0            0   0  0     0   0     0  \n",
       "sent1       0   0    0          0            0   0  0     0   0     0  \n",
       "sent2       0   0    0          0            0   0  0     0   0     0  \n",
       "sent3       0   0    0          0            0   0  0     0   0     0  \n",
       "sent4       0   0    0          0            0   0  0     0   0     0  \n",
       "sent5       0   0    0          0            0   0  0     0   0     0  \n",
       "sent6       1   1    1          0            0   0  0     0   0     0  \n",
       "sent7       0   0    0          1            1   1  1     1   1     1  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {}\n",
    "for i, sent in enumerate(sentence.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())\n",
    "\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try some Dot Product calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product of sent0 from sent1:  1 \n",
      "dot product of sent0 from sent2:  2 \n",
      "dot product of sent0 from sent3:  2 \n",
      "dot product of sent0 from sent4:  3\n"
     ]
    }
   ],
   "source": [
    "df = df.T\n",
    "print(\"dot product of sent0 from sent1: \", df.sent0.dot(df.sent1), \"\\ndot product of sent0 from sent2: \", df.sent0.dot(df.sent2), \"\\ndot product of sent0 from sent3: \", df.sent0.dot(df.sent2),\"\\ndot product of sent0 from sent4: \", df.sent0.dot(df.sent4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the results, the higher the dot product the more similar the vectors are..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve our vocabulary now if we were to remove all other punctuation. Let's first do that with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'talk',\n",
       " 'with',\n",
       " 'crowds',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'your',\n",
       " 'virtue',\n",
       " 'Or',\n",
       " 'walk',\n",
       " 'with',\n",
       " 'Kings—nor',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'common',\n",
       " 'touch',\n",
       " 'If',\n",
       " 'neither',\n",
       " 'foes',\n",
       " 'nor',\n",
       " 'loving',\n",
       " 'friends',\n",
       " 'can',\n",
       " 'hurt',\n",
       " 'you',\n",
       " 'If',\n",
       " 'all',\n",
       " 'men',\n",
       " 'count',\n",
       " 'with',\n",
       " 'you',\n",
       " 'but',\n",
       " 'none',\n",
       " 'too',\n",
       " 'much',\n",
       " 'If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'fill',\n",
       " 'the',\n",
       " 'unforgiving',\n",
       " 'minute',\n",
       " 'With',\n",
       " 'sixty',\n",
       " 'seconds’',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'distance',\n",
       " 'run',\n",
       " 'Yours',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Earth',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'that’s',\n",
       " 'in',\n",
       " 'it',\n",
       " 'And—which',\n",
       " 'is',\n",
       " 'more—you’ll',\n",
       " 'be',\n",
       " 'a',\n",
       " 'Man',\n",
       " 'my',\n",
       " 'son',\n",
       " '']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this seems great... we might still face issues with different characters that are not anticipated. So we usually use an existing NLP related tokenizer to do this job. Let's try the NLTK lib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also supports regular expressions:\n",
    "\n",
    "### RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'talk',\n",
       " 'with',\n",
       " 'crowds',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'your',\n",
       " 'virtue',\n",
       " ',',\n",
       " 'Or',\n",
       " 'walk',\n",
       " 'with',\n",
       " 'Kings',\n",
       " '—nor',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'common',\n",
       " 'touch',\n",
       " '.',\n",
       " 'If',\n",
       " 'neither',\n",
       " 'foes',\n",
       " 'nor',\n",
       " 'loving',\n",
       " 'friends',\n",
       " 'can',\n",
       " 'hurt',\n",
       " 'you',\n",
       " ',',\n",
       " 'If',\n",
       " 'all',\n",
       " 'men',\n",
       " 'count',\n",
       " 'with',\n",
       " 'you',\n",
       " ',',\n",
       " 'but',\n",
       " 'none',\n",
       " 'too',\n",
       " 'much',\n",
       " '.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'fill',\n",
       " 'the',\n",
       " 'unforgiving',\n",
       " 'minute',\n",
       " ',',\n",
       " 'With',\n",
       " 'sixty',\n",
       " 'seconds',\n",
       " '’',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'distance',\n",
       " 'run',\n",
       " ',',\n",
       " 'Yours',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Earth',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'that',\n",
       " '’s',\n",
       " 'in',\n",
       " 'it',\n",
       " ',',\n",
       " 'And',\n",
       " '—which',\n",
       " 'is',\n",
       " 'more',\n",
       " '—you’ll',\n",
       " 'be',\n",
       " 'a',\n",
       " 'Man',\n",
       " ',',\n",
       " 'my',\n",
       " 'son',\n",
       " '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RegexpTokenizer here\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then there are other, more specialised tokenizers:\n",
    "\n",
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'talk',\n",
       " 'with',\n",
       " 'crowds',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'your',\n",
       " 'virtue',\n",
       " ',',\n",
       " 'Or',\n",
       " 'walk',\n",
       " 'with',\n",
       " 'Kings—nor',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'common',\n",
       " 'touch.',\n",
       " 'If',\n",
       " 'neither',\n",
       " 'foes',\n",
       " 'nor',\n",
       " 'loving',\n",
       " 'friends',\n",
       " 'can',\n",
       " 'hurt',\n",
       " 'you',\n",
       " ',',\n",
       " 'If',\n",
       " 'all',\n",
       " 'men',\n",
       " 'count',\n",
       " 'with',\n",
       " 'you',\n",
       " ',',\n",
       " 'but',\n",
       " 'none',\n",
       " 'too',\n",
       " 'much.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'fill',\n",
       " 'the',\n",
       " 'unforgiving',\n",
       " 'minute',\n",
       " ',',\n",
       " 'With',\n",
       " 'sixty',\n",
       " 'seconds’',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'distance',\n",
       " 'run',\n",
       " ',',\n",
       " 'Yours',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Earth',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'that’s',\n",
       " 'in',\n",
       " 'it',\n",
       " ',',\n",
       " 'And—which',\n",
       " 'is',\n",
       " 'more—you’ll',\n",
       " 'be',\n",
       " 'a',\n",
       " 'Man',\n",
       " ',',\n",
       " 'my',\n",
       " 'son',\n",
       " '!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TreebankWordTokenizer here\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now let's use the regular expression special word pattern w, so as to have control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'can', 'talk', 'with', 'crowds', 'and', 'keep', 'your', 'virtue', 'Or', 'walk', 'with', 'Kings', 'nor', 'lose', 'the', 'common', 'touch', 'If', 'neither', 'foes', 'nor', 'loving', 'friends', 'can', 'hurt', 'you', 'If', 'all', 'men', 'count', 'with', 'you', 'but', 'none', 'too', 'much', 'If', 'you', 'can', 'fill', 'the', 'unforgiving', 'minute', 'With', 'sixty', 'seconds', 'worth', 'of', 'distance', 'run', 'Yours', 'is', 'the', 'Earth', 'and', 'everything', 'that', 's', 'in', 'it', 'And', 'which', 'is', 'more', 'you', 'll', 'be', 'a', 'Man', 'my', 'son']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can try out different tokenizers from other libraries to make note of the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If you can talk with crowds and keep your virtue,\\nOr walk with Kings—nor lose the common touch.', 'If neither foes nor loving friends can hurt you,\\nIf all men count with you, but none too much.', 'If you can fill the unforgiving minute,\\nWith sixty seconds’ worth of distance run,\\nYours is the Earth and everything that’s in it,\\nAnd—which is more—you’ll be a Man, my son!']\n"
     ]
    }
   ],
   "source": [
    "# PunktSentenceTokenizer here\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "tokenizer = PunktSentenceTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 't', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'c', 'r', 'o', 'w', 'd', 's', ' ', 'a', 'n', 'd', ' ', 'k', 'e', 'e', 'p', ' ', 'y', 'o', 'u', 'r', ' ', 'v', 'i', 'r', 't', 'u', 'e', ',', '\\n', 'O', 'r', ' ', 'w', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'K', 'i', 'n', 'g', 's', '—', 'n', 'o', 'r', ' ', 'l', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'm', 'm', 'o', 'n', ' ', 't', 'o', 'u', 'c', 'h', '.', '\\n', 'I', 'f', ' ', 'n', 'e', 'i', 't', 'h', 'e', 'r', ' ', 'f', 'o', 'e', 's', ' ', 'n', 'o', 'r', ' ', 'l', 'o', 'v', 'i', 'n', 'g', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 's', ' ', 'c', 'a', 'n', ' ', 'h', 'u', 'r', 't', ' ', 'y', 'o', 'u', ',', '\\n', 'I', 'f', ' ', 'a', 'l', 'l', ' ', 'm', 'e', 'n', ' ', 'c', 'o', 'u', 'n', 't', ' ', 'w', 'i', 't', 'h', ' ', 'y', 'o', 'u', ',', ' ', 'b', 'u', 't', ' ', 'n', 'o', 'n', 'e', ' ', 't', 'o', 'o', ' ', 'm', 'u', 'c', 'h', '.', '\\n', 'I', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'f', 'i', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'u', 'n', 'f', 'o', 'r', 'g', 'i', 'v', 'i', 'n', 'g', ' ', 'm', 'i', 'n', 'u', 't', 'e', ',', '\\n', 'W', 'i', 't', 'h', ' ', 's', 'i', 'x', 't', 'y', ' ', 's', 'e', 'c', 'o', 'n', 'd', 's', '’', ' ', 'w', 'o', 'r', 't', 'h', ' ', 'o', 'f', ' ', 'd', 'i', 's', 't', 'a', 'n', 'c', 'e', ' ', 'r', 'u', 'n', ',', '\\n', 'Y', 'o', 'u', 'r', 's', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'E', 'a', 'r', 't', 'h', ' ', 'a', 'n', 'd', ' ', 'e', 'v', 'e', 'r', 'y', 't', 'h', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', '’', 's', ' ', 'i', 'n', ' ', 'i', 't', ',', '\\n', 'A', 'n', 'd', '—', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 's', ' ', 'm', 'o', 'r', 'e', '—', 'y', 'o', 'u', '’', 'l', 'l', ' ', 'b', 'e', ' ', 'a', ' ', 'M', 'a', 'n', ',', ' ', 'm', 'y', ' ', 's', 'o', 'n', '!']\n"
     ]
    }
   ],
   "source": [
    "# MWETokenizer here\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "tokenizer = MWETokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'can', 'talk', 'with', 'crowds', 'and', 'keep', 'your', 'virtue', ',', 'Or', 'walk', 'with', 'Kings', '—', 'nor', 'lose', 'the', 'common', 'touch', '.', 'If', 'neither', 'foes', 'nor', 'loving', 'friends', 'can', 'hurt', 'you', ',', 'If', 'all', 'men', 'count', 'with', 'you', ',', 'but', 'none', 'too', 'much', '.', 'If', 'you', 'can', 'fill', 'the', 'unforgiving', 'minute', ',', 'With', 'sixty', 'seconds', '’', 'worth', 'of', 'distance', 'run', ',', 'Yours', 'is', 'the', 'Earth', 'and', 'everything', 'that', '’', 's', 'in', 'it', ',', 'And', '—', 'which', 'is', 'more', '—', 'you', '’', 'll', 'be', 'a', 'Man', ',', 'my', 'son', '!']\n"
     ]
    }
   ],
   "source": [
    "# TweetTokenizer here\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# n-Gram Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('If', 'you'),\n",
       " ('you', 'can'),\n",
       " ('can', 'talk'),\n",
       " ('talk', 'with'),\n",
       " ('with', 'crowds'),\n",
       " ('crowds', 'and'),\n",
       " ('and', 'keep'),\n",
       " ('keep', 'your'),\n",
       " ('your', 'virtue'),\n",
       " ('virtue', ','),\n",
       " (',', 'Or'),\n",
       " ('Or', 'walk'),\n",
       " ('walk', 'with'),\n",
       " ('with', 'Kings'),\n",
       " ('Kings', '—'),\n",
       " ('—', 'nor'),\n",
       " ('nor', 'lose'),\n",
       " ('lose', 'the'),\n",
       " ('the', 'common'),\n",
       " ('common', 'touch'),\n",
       " ('touch', '.'),\n",
       " ('.', 'If'),\n",
       " ('If', 'neither'),\n",
       " ('neither', 'foes'),\n",
       " ('foes', 'nor'),\n",
       " ('nor', 'loving'),\n",
       " ('loving', 'friends'),\n",
       " ('friends', 'can'),\n",
       " ('can', 'hurt'),\n",
       " ('hurt', 'you'),\n",
       " ('you', ','),\n",
       " (',', 'If'),\n",
       " ('If', 'all'),\n",
       " ('all', 'men'),\n",
       " ('men', 'count'),\n",
       " ('count', 'with'),\n",
       " ('with', 'you'),\n",
       " ('you', ','),\n",
       " (',', 'but'),\n",
       " ('but', 'none'),\n",
       " ('none', 'too'),\n",
       " ('too', 'much'),\n",
       " ('much', '.'),\n",
       " ('.', 'If'),\n",
       " ('If', 'you'),\n",
       " ('you', 'can'),\n",
       " ('can', 'fill'),\n",
       " ('fill', 'the'),\n",
       " ('the', 'unforgiving'),\n",
       " ('unforgiving', 'minute'),\n",
       " ('minute', ','),\n",
       " (',', 'With'),\n",
       " ('With', 'sixty'),\n",
       " ('sixty', 'seconds'),\n",
       " ('seconds', '’'),\n",
       " ('’', 'worth'),\n",
       " ('worth', 'of'),\n",
       " ('of', 'distance'),\n",
       " ('distance', 'run'),\n",
       " ('run', ','),\n",
       " (',', 'Yours'),\n",
       " ('Yours', 'is'),\n",
       " ('is', 'the'),\n",
       " ('the', 'Earth'),\n",
       " ('Earth', 'and'),\n",
       " ('and', 'everything'),\n",
       " ('everything', 'that'),\n",
       " ('that', '’'),\n",
       " ('’', 's'),\n",
       " ('s', 'in'),\n",
       " ('in', 'it'),\n",
       " ('it', ','),\n",
       " (',', 'And'),\n",
       " ('And', '—'),\n",
       " ('—', 'which'),\n",
       " ('which', 'is'),\n",
       " ('is', 'more'),\n",
       " ('more', '—'),\n",
       " ('—', 'you'),\n",
       " ('you', '’'),\n",
       " ('’', 'll'),\n",
       " ('ll', 'be'),\n",
       " ('be', 'a'),\n",
       " ('a', 'Man'),\n",
       " ('Man', ','),\n",
       " (',', 'my'),\n",
       " ('my', 'son'),\n",
       " ('son', '!')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('If', 'you', 'can'),\n",
       " ('you', 'can', 'talk'),\n",
       " ('can', 'talk', 'with'),\n",
       " ('talk', 'with', 'crowds'),\n",
       " ('with', 'crowds', 'and'),\n",
       " ('crowds', 'and', 'keep'),\n",
       " ('and', 'keep', 'your'),\n",
       " ('keep', 'your', 'virtue'),\n",
       " ('your', 'virtue', ','),\n",
       " ('virtue', ',', 'Or'),\n",
       " (',', 'Or', 'walk'),\n",
       " ('Or', 'walk', 'with'),\n",
       " ('walk', 'with', 'Kings'),\n",
       " ('with', 'Kings', '—'),\n",
       " ('Kings', '—', 'nor'),\n",
       " ('—', 'nor', 'lose'),\n",
       " ('nor', 'lose', 'the'),\n",
       " ('lose', 'the', 'common'),\n",
       " ('the', 'common', 'touch'),\n",
       " ('common', 'touch', '.'),\n",
       " ('touch', '.', 'If'),\n",
       " ('.', 'If', 'neither'),\n",
       " ('If', 'neither', 'foes'),\n",
       " ('neither', 'foes', 'nor'),\n",
       " ('foes', 'nor', 'loving'),\n",
       " ('nor', 'loving', 'friends'),\n",
       " ('loving', 'friends', 'can'),\n",
       " ('friends', 'can', 'hurt'),\n",
       " ('can', 'hurt', 'you'),\n",
       " ('hurt', 'you', ','),\n",
       " ('you', ',', 'If'),\n",
       " (',', 'If', 'all'),\n",
       " ('If', 'all', 'men'),\n",
       " ('all', 'men', 'count'),\n",
       " ('men', 'count', 'with'),\n",
       " ('count', 'with', 'you'),\n",
       " ('with', 'you', ','),\n",
       " ('you', ',', 'but'),\n",
       " (',', 'but', 'none'),\n",
       " ('but', 'none', 'too'),\n",
       " ('none', 'too', 'much'),\n",
       " ('too', 'much', '.'),\n",
       " ('much', '.', 'If'),\n",
       " ('.', 'If', 'you'),\n",
       " ('If', 'you', 'can'),\n",
       " ('you', 'can', 'fill'),\n",
       " ('can', 'fill', 'the'),\n",
       " ('fill', 'the', 'unforgiving'),\n",
       " ('the', 'unforgiving', 'minute'),\n",
       " ('unforgiving', 'minute', ','),\n",
       " ('minute', ',', 'With'),\n",
       " (',', 'With', 'sixty'),\n",
       " ('With', 'sixty', 'seconds'),\n",
       " ('sixty', 'seconds', '’'),\n",
       " ('seconds', '’', 'worth'),\n",
       " ('’', 'worth', 'of'),\n",
       " ('worth', 'of', 'distance'),\n",
       " ('of', 'distance', 'run'),\n",
       " ('distance', 'run', ','),\n",
       " ('run', ',', 'Yours'),\n",
       " (',', 'Yours', 'is'),\n",
       " ('Yours', 'is', 'the'),\n",
       " ('is', 'the', 'Earth'),\n",
       " ('the', 'Earth', 'and'),\n",
       " ('Earth', 'and', 'everything'),\n",
       " ('and', 'everything', 'that'),\n",
       " ('everything', 'that', '’'),\n",
       " ('that', '’', 's'),\n",
       " ('’', 's', 'in'),\n",
       " ('s', 'in', 'it'),\n",
       " ('in', 'it', ','),\n",
       " ('it', ',', 'And'),\n",
       " (',', 'And', '—'),\n",
       " ('And', '—', 'which'),\n",
       " ('—', 'which', 'is'),\n",
       " ('which', 'is', 'more'),\n",
       " ('is', 'more', '—'),\n",
       " ('more', '—', 'you'),\n",
       " ('—', 'you', '’'),\n",
       " ('you', '’', 'll'),\n",
       " ('’', 'll', 'be'),\n",
       " ('ll', 'be', 'a'),\n",
       " ('be', 'a', 'Man'),\n",
       " ('a', 'Man', ','),\n",
       " ('Man', ',', 'my'),\n",
       " (',', 'my', 'son'),\n",
       " ('my', 'son', '!')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include the n-grams as a string rather than as tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If you', 'you can', 'can talk', 'talk with', 'with crowds', 'crowds and', 'and keep', 'keep your', 'your virtue', 'virtue ,', ', Or', 'Or walk', 'walk with', 'with Kings', 'Kings —', '— nor', 'nor lose', 'lose the', 'the common', 'common touch', 'touch .', '. If', 'If neither', 'neither foes', 'foes nor', 'nor loving', 'loving friends', 'friends can', 'can hurt', 'hurt you', 'you ,', ', If', 'If all', 'all men', 'men count', 'count with', 'with you', 'you ,', ', but', 'but none', 'none too', 'too much', 'much .', '. If', 'If you', 'you can', 'can fill', 'fill the', 'the unforgiving', 'unforgiving minute', 'minute ,', ', With', 'With sixty', 'sixty seconds', 'seconds ’', '’ worth', 'worth of', 'of distance', 'distance run', 'run ,', ', Yours', 'Yours is', 'is the', 'the Earth', 'Earth and', 'and everything', 'everything that', 'that ’', '’ s', 's in', 'in it', 'it ,', ', And', 'And —', '— which', 'which is', 'is more', 'more —', '— you', 'you ’', '’ ll', 'll be', 'be a', 'a Man', 'Man ,', ', my', 'my son', 'son !']\n",
      "\n",
      "\n",
      "\n",
      "['If you can', 'you can talk', 'can talk with', 'talk with crowds', 'with crowds and', 'crowds and keep', 'and keep your', 'keep your virtue', 'your virtue ,', 'virtue , Or', ', Or walk', 'Or walk with', 'walk with Kings', 'with Kings —', 'Kings — nor', '— nor lose', 'nor lose the', 'lose the common', 'the common touch', 'common touch .', 'touch . If', '. If neither', 'If neither foes', 'neither foes nor', 'foes nor loving', 'nor loving friends', 'loving friends can', 'friends can hurt', 'can hurt you', 'hurt you ,', 'you , If', ', If all', 'If all men', 'all men count', 'men count with', 'count with you', 'with you ,', 'you , but', ', but none', 'but none too', 'none too much', 'too much .', 'much . If', '. If you', 'If you can', 'you can fill', 'can fill the', 'fill the unforgiving', 'the unforgiving minute', 'unforgiving minute ,', 'minute , With', ', With sixty', 'With sixty seconds', 'sixty seconds ’', 'seconds ’ worth', '’ worth of', 'worth of distance', 'of distance run', 'distance run ,', 'run , Yours', ', Yours is', 'Yours is the', 'is the Earth', 'the Earth and', 'Earth and everything', 'and everything that', 'everything that ’', 'that ’ s', '’ s in', 's in it', 'in it ,', 'it , And', ', And —', 'And — which', '— which is', 'which is more', 'is more —', 'more — you', '— you ’', 'you ’ ll', '’ ll be', 'll be a', 'be a Man', 'a Man ,', 'Man , my', ', my son', 'my son !']\n"
     ]
    }
   ],
   "source": [
    "bigrams = [\" \".join(x) for x in list(ngrams(tokens, 2))]\n",
    "print(bigrams)\n",
    "trigrams = [\" \".join(x) for x in list(ngrams(tokens, 3))]\n",
    "print(\"\\n\\n\")\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's download the list from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nn007/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then check it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 179\n",
      "\n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(\"number of stopwords:\", len(stop_words))\n",
    "print(\"\\n\",stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other libs have different stopwords. Let's see a much larger set from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 318\n",
      "\n",
      " frozenset({'yet', 'over', 'towards', 'eleven', 'yours', 'whereby', 'ever', 'on', 'forty', 'con', 'became', 'yourself', 'its', 'hereupon', 'beside', 'neither', 'see', 'anyhow', 'same', 'these', 'thereupon', 'everything', 'often', 'back', 'under', 'more', 'why', 'the', 'seemed', 'whenever', 'were', 'then', 'and', 'move', 'anyway', 'seems', 'de', 'all', 'could', 'it', 'bill', 'of', 'co', 'at', 'being', 'besides', 'whence', 'themselves', 'such', 'which', 'afterwards', 'seem', 'off', 'our', 'after', 'may', 'anything', 'whom', 'interest', 'somehow', 'be', 'part', 'what', 'else', 'ltd', 'can', 'somewhere', 'will', 'itself', 'take', 'sixty', 'therein', 'about', 'well', 'among', 'next', 'due', 'his', 'also', 'always', 'couldnt', 'your', 'otherwise', 'into', 'third', 'are', 'ie', 'whereas', 'done', 'call', 'only', 'until', 'whoever', 'how', 'still', 'both', 'very', 'thence', 'moreover', 'anyone', 'therefore', 'side', 'he', 'beyond', 'amount', 'sometimes', 'most', 'whether', 'rather', 'too', 'per', 'before', 'thereafter', 'go', 'each', 'around', 'nor', 'bottom', 'thereby', 'latter', 'another', 'amoungst', 'a', 'empty', 'become', 'either', 'when', 'nowhere', 'them', 'during', 'however', 'against', 'everyone', 'my', 'down', 'fifty', 'as', 'top', 'she', 'their', 'thus', 'while', 'her', 'two', 'last', 'without', 'above', 'an', 'elsewhere', 'give', 'toward', 'out', 're', 'enough', 'fifteen', 'was', 'there', 'if', 'former', 'hence', 'or', 'in', 'keep', 'namely', 'fill', 'ours', 'whatever', 'whose', 'sincere', 'you', 'have', 'whole', 'him', 'since', 'across', 'for', 'made', 'put', 'much', 'none', 'show', 'am', 'whither', 'hundred', 'no', 'every', 'now', 'detail', 'throughout', 'amongst', 'hereafter', 'eight', 'name', 'we', 'cannot', 'thru', 'further', 'almost', 'myself', 'mine', 'to', 'even', 'someone', 'though', 'many', 'several', 'between', 'least', 'behind', 'own', 'must', 'cry', 'along', 'nobody', 'once', 'un', 'whereafter', 'alone', 'do', 'has', 'through', 'ourselves', 'those', 'eg', 'via', 'four', 'noone', 'fire', 'yourselves', 'meanwhile', 'not', 'mill', 'system', 'seeming', 'beforehand', 'although', 'one', 'latterly', 'upon', 'few', 'they', 'full', 'becoming', 'onto', 'please', 'other', 'had', 'twelve', 'formerly', 'this', 'should', 'wherever', 'is', 'wherein', 'cant', 'anywhere', 'hasnt', 'hereby', 'hers', 'within', 'up', 'perhaps', 'i', 'nothing', 'others', 'six', 'so', 'serious', 'here', 'below', 'nine', 'something', 'thick', 'because', 'whereupon', 'indeed', 'front', 'except', 'get', 'by', 'himself', 'together', 'would', 'describe', 'me', 'everywhere', 'ten', 'again', 'becomes', 'herein', 'herself', 'less', 'that', 'etc', 'any', 'nevertheless', 'from', 'but', 'some', 'than', 'never', 'with', 'thin', 'already', 'been', 'first', 'might', 'us', 'three', 'inc', 'five', 'twenty', 'sometime', 'where', 'mostly', 'who', 'find', 'found'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "\n",
    "print(\"number of stopwords:\", len(sklearn_stop_words))\n",
    "print(\"\\n\",sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that although there are more stopwords in sklearn, nltk has words that are not contained in sklearn. So we might want to join the two lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normalizing the text we could do something as simple as making sure all words are lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', 'can', 'talk', 'with', 'crowds', 'and', 'keep', 'your', 'virtue', ',', 'or', 'walk', 'with', 'kings', '—', 'nor', 'lose', 'the', 'common', 'touch', '.', 'if', 'neither', 'foes', 'nor', 'loving', 'friends', 'can', 'hurt', 'you', ',', 'if', 'all', 'men', 'count', 'with', 'you', ',', 'but', 'none', 'too', 'much', '.', 'if', 'you', 'can', 'fill', 'the', 'unforgiving', 'minute', ',', 'with', 'sixty', 'seconds', '’', 'worth', 'of', 'distance', 'run', ',', 'yours', 'is', 'the', 'earth', 'and', 'everything', 'that', '’', 's', 'in', 'it', ',', 'and', '—', 'which', 'is', 'more', '—', 'you', '’', 'll', 'be', 'a', 'man', ',', 'my', 'son', '!']\n"
     ]
    }
   ],
   "source": [
    "norm_tokens = [x.lower() for x in tokens]\n",
    "print(norm_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For stemming the words, we can use NLTK again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', 'can', 'talk', 'with', 'crowd', 'and', 'keep', 'your', 'virtu', ',', 'or', 'walk', 'with', 'king', '—', 'nor', 'lose', 'the', 'common', 'touch', '.', 'if', 'neither', 'foe', 'nor', 'love', 'friend', 'can', 'hurt', 'you', ',', 'if', 'all', 'men', 'count', 'with', 'you', ',', 'but', 'none', 'too', 'much', '.', 'if', 'you', 'can', 'fill', 'the', 'unforgiv', 'minut', ',', 'with', 'sixti', 'second', '’', 'worth', 'of', 'distanc', 'run', ',', 'your', 'is', 'the', 'earth', 'and', 'everyth', 'that', '’', 's', 'in', 'it', ',', 'and', '—', 'which', 'is', 'more', '—', 'you', '’', 'll', 'be', 'a', 'man', ',', 'my', 'son', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stem_tokens = [stemmer.stem(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lemmatising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nn007/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stem_tokens = [lemmatizer.lemmatize(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences we have has no issues with the lemma... but look into the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", 'a')) # declaring the POS as adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't include the POS, the nltk library with wordnet does not work well. So let's try fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    # now we need to convert from nltk to wordnet POS notations (for compatibility reasons)\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # return and default to noun if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_tokens = [lemmatizer.lemmatize(x, pos=get_wordnet_pos(x)) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the words now we are getting more counts for our bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature-vector creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bow = Counter(stem_tokens)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the most frequent 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's remove the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_tokens = [x for x in stem_tokens if x not in stop_words]\n",
    "count = Counter(no_stop_tokens)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally... let's make our feature vector using the frequency ratio (term count / total number of terms in the doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector = []\n",
    "doc_length = len(no_stop_tokens)\n",
    "for key, value in count.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "\n",
    "print(document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
