{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's set some text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"If you can talk with crowds and keep your virtue,\\nOr walk with Kings—nor lose the common touch.\\nIf neither foes nor loving friends can hurt you,\\nIf all men count with you, but none too much.\\nIf you can fill the unforgiving minute,\\nWith sixty seconds’ worth of distance run,\\nYours is the Earth and everything that’s in it,\\nAnd—which is more—you’ll be a Man, my son!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to split the text based on spaces (default function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'talk',\n",
       " 'with',\n",
       " 'crowds',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'your',\n",
       " 'virtue,',\n",
       " 'Or',\n",
       " 'walk',\n",
       " 'with',\n",
       " 'Kings—nor',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'common',\n",
       " 'touch.',\n",
       " 'If',\n",
       " 'neither',\n",
       " 'foes',\n",
       " 'nor',\n",
       " 'loving',\n",
       " 'friends',\n",
       " 'can',\n",
       " 'hurt',\n",
       " 'you,',\n",
       " 'If',\n",
       " 'all',\n",
       " 'men',\n",
       " 'count',\n",
       " 'with',\n",
       " 'you,',\n",
       " 'but',\n",
       " 'none',\n",
       " 'too',\n",
       " 'much.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'fill',\n",
       " 'the',\n",
       " 'unforgiving',\n",
       " 'minute,',\n",
       " 'With',\n",
       " 'sixty',\n",
       " 'seconds’',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'distance',\n",
       " 'run,',\n",
       " 'Yours',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Earth',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'that’s',\n",
       " 'in',\n",
       " 'it,',\n",
       " 'And—which',\n",
       " 'is',\n",
       " 'more—you’ll',\n",
       " 'be',\n",
       " 'a',\n",
       " 'Man,',\n",
       " 'my',\n",
       " 'son!']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that some words are not well separated from punctuation.\n",
    "So let's try to remove those characters... but before that, let's create a quick feature vector first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And—which',\n",
       " 'Earth',\n",
       " 'If',\n",
       " 'Kings—nor',\n",
       " 'Man,',\n",
       " 'Or',\n",
       " 'With',\n",
       " 'Yours',\n",
       " 'a',\n",
       " 'all',\n",
       " 'and',\n",
       " 'be',\n",
       " 'but',\n",
       " 'can',\n",
       " 'common',\n",
       " 'count',\n",
       " 'crowds',\n",
       " 'distance',\n",
       " 'everything',\n",
       " 'fill',\n",
       " 'foes',\n",
       " 'friends',\n",
       " 'hurt',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it,',\n",
       " 'keep',\n",
       " 'lose',\n",
       " 'loving',\n",
       " 'men',\n",
       " 'minute,',\n",
       " 'more—you’ll',\n",
       " 'much.',\n",
       " 'my',\n",
       " 'neither',\n",
       " 'none',\n",
       " 'nor',\n",
       " 'of',\n",
       " 'run,',\n",
       " 'seconds’',\n",
       " 'sixty',\n",
       " 'son!',\n",
       " 'talk',\n",
       " 'that’s',\n",
       " 'the',\n",
       " 'too',\n",
       " 'touch.',\n",
       " 'unforgiving',\n",
       " 'virtue,',\n",
       " 'walk',\n",
       " 'with',\n",
       " 'worth',\n",
       " 'you',\n",
       " 'you,',\n",
       " 'your']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sorted(sentence.split()) # splitting based on spaces\n",
    "vocab = sorted(set(tokens)) # sorting and removing duplicates by using set()\n",
    "vocab # just printing the vocab so we can look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the order is: numbers first, followerd by capital letters, and then lower case letters (all alphabetically sorted). We also note that some repeating words appear only once in the vocabulary list. Let's compare the size of the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 68\n",
      "vocab: 55\n"
     ]
    }
   ],
   "source": [
    "tokens_len = len(tokens)\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "print(\"tokens:\", tokens_len)\n",
    "print(\"vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and print the matrix of tokens against vocabulary. We will use the numpy lib for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.zeros((tokens_len, vocab_len), int)\n",
    "for i, token in enumerate(tokens):\n",
    "    matrix[i, vocab.index(token)] = 1\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more readable, we can use Pandas and DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>And—which</th>\n",
       "      <th>Earth</th>\n",
       "      <th>If</th>\n",
       "      <th>Kings—nor</th>\n",
       "      <th>Man,</th>\n",
       "      <th>Or</th>\n",
       "      <th>With</th>\n",
       "      <th>Yours</th>\n",
       "      <th>a</th>\n",
       "      <th>all</th>\n",
       "      <th>...</th>\n",
       "      <th>too</th>\n",
       "      <th>touch.</th>\n",
       "      <th>unforgiving</th>\n",
       "      <th>virtue,</th>\n",
       "      <th>walk</th>\n",
       "      <th>with</th>\n",
       "      <th>worth</th>\n",
       "      <th>you</th>\n",
       "      <th>you,</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>And—which</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Earth</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>If</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you,</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you,</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           And—which  Earth  If  Kings—nor  Man,  Or  With  Yours  a  all  \\\n",
       "And—which          1      0   0          0     0   0     0      0  0    0   \n",
       "Earth              0      1   0          0     0   0     0      0  0    0   \n",
       "If                 0      0   1          0     0   0     0      0  0    0   \n",
       "If                 0      0   1          0     0   0     0      0  0    0   \n",
       "If                 0      0   1          0     0   0     0      0  0    0   \n",
       "...              ...    ...  ..        ...   ...  ..   ...    ... ..  ...   \n",
       "you                0      0   0          0     0   0     0      0  0    0   \n",
       "you                0      0   0          0     0   0     0      0  0    0   \n",
       "you,               0      0   0          0     0   0     0      0  0    0   \n",
       "you,               0      0   0          0     0   0     0      0  0    0   \n",
       "your               0      0   0          0     0   0     0      0  0    0   \n",
       "\n",
       "           ...  too  touch.  unforgiving  virtue,  walk  with  worth  you  \\\n",
       "And—which  ...    0       0            0        0     0     0      0    0   \n",
       "Earth      ...    0       0            0        0     0     0      0    0   \n",
       "If         ...    0       0            0        0     0     0      0    0   \n",
       "If         ...    0       0            0        0     0     0      0    0   \n",
       "If         ...    0       0            0        0     0     0      0    0   \n",
       "...        ...  ...     ...          ...      ...   ...   ...    ...  ...   \n",
       "you        ...    0       0            0        0     0     0      0    1   \n",
       "you        ...    0       0            0        0     0     0      0    1   \n",
       "you,       ...    0       0            0        0     0     0      0    0   \n",
       "you,       ...    0       0            0        0     0     0      0    0   \n",
       "your       ...    0       0            0        0     0     0      0    0   \n",
       "\n",
       "           you,  your  \n",
       "And—which     0     0  \n",
       "Earth         0     0  \n",
       "If            0     0  \n",
       "If            0     0  \n",
       "If            0     0  \n",
       "...         ...   ...  \n",
       "you           0     0  \n",
       "you           0     0  \n",
       "you,          1     0  \n",
       "you,          1     0  \n",
       "your          0     1  \n",
       "\n",
       "[68 rows x 55 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(matrix, columns=vocab, index=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lot clearer.\n",
    "\n",
    "Let's now build the bag of words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And—which', 1),\n",
       " ('Earth', 1),\n",
       " ('If', 1),\n",
       " ('Kings—nor', 1),\n",
       " ('Man,', 1),\n",
       " ('Or', 1),\n",
       " ('With', 1),\n",
       " ('Yours', 1),\n",
       " ('a', 1),\n",
       " ('all', 1),\n",
       " ('and', 1),\n",
       " ('be', 1),\n",
       " ('but', 1),\n",
       " ('can', 1),\n",
       " ('common', 1),\n",
       " ('count', 1),\n",
       " ('crowds', 1),\n",
       " ('distance', 1),\n",
       " ('everything', 1),\n",
       " ('fill', 1),\n",
       " ('foes', 1),\n",
       " ('friends', 1),\n",
       " ('hurt', 1),\n",
       " ('in', 1),\n",
       " ('is', 1),\n",
       " ('it,', 1),\n",
       " ('keep', 1),\n",
       " ('lose', 1),\n",
       " ('loving', 1),\n",
       " ('men', 1),\n",
       " ('minute,', 1),\n",
       " ('more—you’ll', 1),\n",
       " ('much.', 1),\n",
       " ('my', 1),\n",
       " ('neither', 1),\n",
       " ('none', 1),\n",
       " ('nor', 1),\n",
       " ('of', 1),\n",
       " ('run,', 1),\n",
       " ('seconds’', 1),\n",
       " ('sixty', 1),\n",
       " ('son!', 1),\n",
       " ('talk', 1),\n",
       " ('that’s', 1),\n",
       " ('the', 1),\n",
       " ('too', 1),\n",
       " ('touch.', 1),\n",
       " ('unforgiving', 1),\n",
       " ('virtue,', 1),\n",
       " ('walk', 1),\n",
       " ('with', 1),\n",
       " ('worth', 1),\n",
       " ('you', 1),\n",
       " ('you,', 1),\n",
       " ('your', 1)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = {} # setting this up as a dictionary\n",
    "\n",
    "for token in tokens:\n",
    "    bow[token] = 1\n",
    "\n",
    "sorted(bow.items()) # lets print it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since bow is a dictionary, we see that the same words will not duplicate.\n",
    "\n",
    "Pandas also has a more efficient form of a dictionary called Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>And—which</th>\n",
       "      <th>Earth</th>\n",
       "      <th>If</th>\n",
       "      <th>Kings—nor</th>\n",
       "      <th>Man,</th>\n",
       "      <th>Or</th>\n",
       "      <th>With</th>\n",
       "      <th>Yours</th>\n",
       "      <th>a</th>\n",
       "      <th>all</th>\n",
       "      <th>...</th>\n",
       "      <th>too</th>\n",
       "      <th>touch.</th>\n",
       "      <th>unforgiving</th>\n",
       "      <th>virtue,</th>\n",
       "      <th>walk</th>\n",
       "      <th>with</th>\n",
       "      <th>worth</th>\n",
       "      <th>you</th>\n",
       "      <th>you,</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      And—which  Earth  If  Kings—nor  Man,  Or  With  Yours  a  all  ...  \\\n",
       "sent          1      1   1          1     1   1     1      1  1    1  ...   \n",
       "\n",
       "      too  touch.  unforgiving  virtue,  walk  with  worth  you  you,  your  \n",
       "sent    1       1            1        1     1     1      1    1     1     1  \n",
       "\n",
       "[1 rows x 55 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in tokens])), columns=['sent']).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>If</th>\n",
       "      <th>you</th>\n",
       "      <th>can</th>\n",
       "      <th>talk</th>\n",
       "      <th>with</th>\n",
       "      <th>crowds</th>\n",
       "      <th>and</th>\n",
       "      <th>keep</th>\n",
       "      <th>your</th>\n",
       "      <th>virtue,</th>\n",
       "      <th>...</th>\n",
       "      <th>that’s</th>\n",
       "      <th>in</th>\n",
       "      <th>it,</th>\n",
       "      <th>And—which</th>\n",
       "      <th>more—you’ll</th>\n",
       "      <th>be</th>\n",
       "      <th>a</th>\n",
       "      <th>Man,</th>\n",
       "      <th>my</th>\n",
       "      <th>son!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       If  you  can  talk  with  crowds  and  keep  your  virtue,  ...  \\\n",
       "sent0   1    1    1     1     1       1    1     1     1        1  ...   \n",
       "sent1   0    0    0     0     1       0    0     0     0        0  ...   \n",
       "sent2   1    0    1     0     0       0    0     0     0        0  ...   \n",
       "sent3   1    0    0     0     1       0    0     0     0        0  ...   \n",
       "sent4   1    1    1     0     0       0    0     0     0        0  ...   \n",
       "sent5   0    0    0     0     0       0    0     0     0        0  ...   \n",
       "sent6   0    0    0     0     0       0    1     0     0        0  ...   \n",
       "sent7   0    0    0     0     0       0    0     0     0        0  ...   \n",
       "\n",
       "       that’s  in  it,  And—which  more—you’ll  be  a  Man,  my  son!  \n",
       "sent0       0   0    0          0            0   0  0     0   0     0  \n",
       "sent1       0   0    0          0            0   0  0     0   0     0  \n",
       "sent2       0   0    0          0            0   0  0     0   0     0  \n",
       "sent3       0   0    0          0            0   0  0     0   0     0  \n",
       "sent4       0   0    0          0            0   0  0     0   0     0  \n",
       "sent5       0   0    0          0            0   0  0     0   0     0  \n",
       "sent6       1   1    1          0            0   0  0     0   0     0  \n",
       "sent7       0   0    0          1            1   1  1     1   1     1  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {}\n",
    "for i, sent in enumerate(sentence.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())\n",
    "\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try some Dot Product calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product of sent0 from sent1:  1 \n",
      "dot product of sent0 from sent2:  2 \n",
      "dot product of sent0 from sent3:  2 \n",
      "dot product of sent0 from sent4:  3\n"
     ]
    }
   ],
   "source": [
    "df = df.T\n",
    "print(\"dot product of sent0 from sent1: \", df.sent0.dot(df.sent1), \"\\ndot product of sent0 from sent2: \", df.sent0.dot(df.sent2), \"\\ndot product of sent0 from sent3: \", df.sent0.dot(df.sent2),\"\\ndot product of sent0 from sent4: \", df.sent0.dot(df.sent4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the results, the higher the dot product the more similar the vectors are..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve our vocabulary now if we were to remove all other punctuation. Let's first do that with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'talk',\n",
       " 'with',\n",
       " 'crowds',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'your',\n",
       " 'virtue',\n",
       " 'Or',\n",
       " 'walk',\n",
       " 'with',\n",
       " 'Kings—nor',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'common',\n",
       " 'touch',\n",
       " 'If',\n",
       " 'neither',\n",
       " 'foes',\n",
       " 'nor',\n",
       " 'loving',\n",
       " 'friends',\n",
       " 'can',\n",
       " 'hurt',\n",
       " 'you',\n",
       " 'If',\n",
       " 'all',\n",
       " 'men',\n",
       " 'count',\n",
       " 'with',\n",
       " 'you',\n",
       " 'but',\n",
       " 'none',\n",
       " 'too',\n",
       " 'much',\n",
       " 'If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'fill',\n",
       " 'the',\n",
       " 'unforgiving',\n",
       " 'minute',\n",
       " 'With',\n",
       " 'sixty',\n",
       " 'seconds’',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'distance',\n",
       " 'run',\n",
       " 'Yours',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Earth',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'that’s',\n",
       " 'in',\n",
       " 'it',\n",
       " 'And—which',\n",
       " 'is',\n",
       " 'more—you’ll',\n",
       " 'be',\n",
       " 'a',\n",
       " 'Man',\n",
       " 'my',\n",
       " 'son',\n",
       " '']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this seems great... we might still face issues with different characters that are not anticipated. So we usually use an existing NLP related tokenizer to do this job. Let's try the NLTK lib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also supports regular expressions:\n",
    "\n",
    "### RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'talk',\n",
       " 'with',\n",
       " 'crowds',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'your',\n",
       " 'virtue',\n",
       " ',',\n",
       " 'Or',\n",
       " 'walk',\n",
       " 'with',\n",
       " 'Kings',\n",
       " '—nor',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'common',\n",
       " 'touch',\n",
       " '.',\n",
       " 'If',\n",
       " 'neither',\n",
       " 'foes',\n",
       " 'nor',\n",
       " 'loving',\n",
       " 'friends',\n",
       " 'can',\n",
       " 'hurt',\n",
       " 'you',\n",
       " ',',\n",
       " 'If',\n",
       " 'all',\n",
       " 'men',\n",
       " 'count',\n",
       " 'with',\n",
       " 'you',\n",
       " ',',\n",
       " 'but',\n",
       " 'none',\n",
       " 'too',\n",
       " 'much',\n",
       " '.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'fill',\n",
       " 'the',\n",
       " 'unforgiving',\n",
       " 'minute',\n",
       " ',',\n",
       " 'With',\n",
       " 'sixty',\n",
       " 'seconds',\n",
       " '’',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'distance',\n",
       " 'run',\n",
       " ',',\n",
       " 'Yours',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Earth',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'that',\n",
       " '’s',\n",
       " 'in',\n",
       " 'it',\n",
       " ',',\n",
       " 'And',\n",
       " '—which',\n",
       " 'is',\n",
       " 'more',\n",
       " '—you’ll',\n",
       " 'be',\n",
       " 'a',\n",
       " 'Man',\n",
       " ',',\n",
       " 'my',\n",
       " 'son',\n",
       " '!']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RegexpTokenizer here\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then there are other, more specialised tokenizers:\n",
    "\n",
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'talk',\n",
       " 'with',\n",
       " 'crowds',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'your',\n",
       " 'virtue',\n",
       " ',',\n",
       " 'Or',\n",
       " 'walk',\n",
       " 'with',\n",
       " 'Kings—nor',\n",
       " 'lose',\n",
       " 'the',\n",
       " 'common',\n",
       " 'touch.',\n",
       " 'If',\n",
       " 'neither',\n",
       " 'foes',\n",
       " 'nor',\n",
       " 'loving',\n",
       " 'friends',\n",
       " 'can',\n",
       " 'hurt',\n",
       " 'you',\n",
       " ',',\n",
       " 'If',\n",
       " 'all',\n",
       " 'men',\n",
       " 'count',\n",
       " 'with',\n",
       " 'you',\n",
       " ',',\n",
       " 'but',\n",
       " 'none',\n",
       " 'too',\n",
       " 'much.',\n",
       " 'If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'fill',\n",
       " 'the',\n",
       " 'unforgiving',\n",
       " 'minute',\n",
       " ',',\n",
       " 'With',\n",
       " 'sixty',\n",
       " 'seconds’',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'distance',\n",
       " 'run',\n",
       " ',',\n",
       " 'Yours',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Earth',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'that’s',\n",
       " 'in',\n",
       " 'it',\n",
       " ',',\n",
       " 'And—which',\n",
       " 'is',\n",
       " 'more—you’ll',\n",
       " 'be',\n",
       " 'a',\n",
       " 'Man',\n",
       " ',',\n",
       " 'my',\n",
       " 'son',\n",
       " '!']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TreebankWordTokenizer here\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now let's use the regular expression special word pattern w, so as to have control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'can', 'talk', 'with', 'crowds', 'and', 'keep', 'your', 'virtue', 'Or', 'walk', 'with', 'Kings', 'nor', 'lose', 'the', 'common', 'touch', 'If', 'neither', 'foes', 'nor', 'loving', 'friends', 'can', 'hurt', 'you', 'If', 'all', 'men', 'count', 'with', 'you', 'but', 'none', 'too', 'much', 'If', 'you', 'can', 'fill', 'the', 'unforgiving', 'minute', 'With', 'sixty', 'seconds', 'worth', 'of', 'distance', 'run', 'Yours', 'is', 'the', 'Earth', 'and', 'everything', 'that', 's', 'in', 'it', 'And', 'which', 'is', 'more', 'you', 'll', 'be', 'a', 'Man', 'my', 'son']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can try out different tokenizers from other libraries to make note of the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'can', 'talk', 'with', 'crowds', 'and', 'keep', 'your', 'virtue', ',', 'Or', 'walk', 'with', 'Kings', '—', 'nor', 'lose', 'the', 'common', 'touch', '.', 'If', 'neither', 'foes', 'nor', 'loving', 'friends', 'can', 'hurt', 'you', ',', 'If', 'all', 'men', 'count', 'with', 'you', ',', 'but', 'none', 'too', 'much', '.', 'If', 'you', 'can', 'fill', 'the', 'unforgiving', 'minute', ',', 'With', 'sixty', 'seconds', '’', 'worth', 'of', 'distance', 'run', ',', 'Yours', 'is', 'the', 'Earth', 'and', 'everything', 'that', '’', 's', 'in', 'it', ',', 'And', '—', 'which', 'is', 'more', '—', 'you', '’', 'll', 'be', 'a', 'Man', ',', 'my', 'son', '!']\n"
     ]
    }
   ],
   "source": [
    "# TweetTokenizer here\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If you can talk with crowds and keep your virtue,\\nOr walk with Kings—nor lose the common touch.', 'If neither foes nor loving friends can hurt you,\\nIf all men count with you, but none too much.', 'If you can fill the unforgiving minute,\\nWith sixty seconds’ worth of distance run,\\nYours is the Earth and everything that’s in it,\\nAnd—which is more—you’ll be a Man, my son!']\n"
     ]
    }
   ],
   "source": [
    "# PunktSentenceTokenizer here\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "tokenizer = PunktSentenceTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 't', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'c', 'r', 'o', 'w', 'd', 's', ' ', 'a', 'n', 'd', ' ', 'k', 'e', 'e', 'p', ' ', 'y', 'o', 'u', 'r', ' ', 'v', 'i', 'r', 't', 'u', 'e', ',', '\\n', 'O', 'r', ' ', 'w', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'K', 'i', 'n', 'g', 's', '—', 'n', 'o', 'r', ' ', 'l', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'm', 'm', 'o', 'n', ' ', 't', 'o', 'u', 'c', 'h', '.', '\\n', 'I', 'f', ' ', 'n', 'e', 'i', 't', 'h', 'e', 'r', ' ', 'f', 'o', 'e', 's', ' ', 'n', 'o', 'r', ' ', 'l', 'o', 'v', 'i', 'n', 'g', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 's', ' ', 'c', 'a', 'n', ' ', 'h', 'u', 'r', 't', ' ', 'y', 'o', 'u', ',', '\\n', 'I', 'f', ' ', 'a', 'l', 'l', ' ', 'm', 'e', 'n', ' ', 'c', 'o', 'u', 'n', 't', ' ', 'w', 'i', 't', 'h', ' ', 'y', 'o', 'u', ',', ' ', 'b', 'u', 't', ' ', 'n', 'o', 'n', 'e', ' ', 't', 'o', 'o', ' ', 'm', 'u', 'c', 'h', '.', '\\n', 'I', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'f', 'i', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'u', 'n', 'f', 'o', 'r', 'g', 'i', 'v', 'i', 'n', 'g', ' ', 'm', 'i', 'n', 'u', 't', 'e', ',', '\\n', 'W', 'i', 't', 'h', ' ', 's', 'i', 'x', 't', 'y', ' ', 's', 'e', 'c', 'o', 'n', 'd', 's', '’', ' ', 'w', 'o', 'r', 't', 'h', ' ', 'o', 'f', ' ', 'd', 'i', 's', 't', 'a', 'n', 'c', 'e', ' ', 'r', 'u', 'n', ',', '\\n', 'Y', 'o', 'u', 'r', 's', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'E', 'a', 'r', 't', 'h', ' ', 'a', 'n', 'd', ' ', 'e', 'v', 'e', 'r', 'y', 't', 'h', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', '’', 's', ' ', 'i', 'n', ' ', 'i', 't', ',', '\\n', 'A', 'n', 'd', '—', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 's', ' ', 'm', 'o', 'r', 'e', '—', 'y', 'o', 'u', '’', 'l', 'l', ' ', 'b', 'e', ' ', 'a', ' ', 'M', 'a', 'n', ',', ' ', 'm', 'y', ' ', 's', 'o', 'n', '!']\n"
     ]
    }
   ],
   "source": [
    "# MWETokenizer here\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "tokenizer = MWETokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# n-Gram Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'f'),\n",
       " ('f', ' '),\n",
       " (' ', 'y'),\n",
       " ('y', 'o'),\n",
       " ('o', 'u'),\n",
       " ('u', ' '),\n",
       " (' ', 'c'),\n",
       " ('c', 'a'),\n",
       " ('a', 'n'),\n",
       " ('n', ' '),\n",
       " (' ', 't'),\n",
       " ('t', 'a'),\n",
       " ('a', 'l'),\n",
       " ('l', 'k'),\n",
       " ('k', ' '),\n",
       " (' ', 'w'),\n",
       " ('w', 'i'),\n",
       " ('i', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', ' '),\n",
       " (' ', 'c'),\n",
       " ('c', 'r'),\n",
       " ('r', 'o'),\n",
       " ('o', 'w'),\n",
       " ('w', 'd'),\n",
       " ('d', 's'),\n",
       " ('s', ' '),\n",
       " (' ', 'a'),\n",
       " ('a', 'n'),\n",
       " ('n', 'd'),\n",
       " ('d', ' '),\n",
       " (' ', 'k'),\n",
       " ('k', 'e'),\n",
       " ('e', 'e'),\n",
       " ('e', 'p'),\n",
       " ('p', ' '),\n",
       " (' ', 'y'),\n",
       " ('y', 'o'),\n",
       " ('o', 'u'),\n",
       " ('u', 'r'),\n",
       " ('r', ' '),\n",
       " (' ', 'v'),\n",
       " ('v', 'i'),\n",
       " ('i', 'r'),\n",
       " ('r', 't'),\n",
       " ('t', 'u'),\n",
       " ('u', 'e'),\n",
       " ('e', ','),\n",
       " (',', '\\n'),\n",
       " ('\\n', 'O'),\n",
       " ('O', 'r'),\n",
       " ('r', ' '),\n",
       " (' ', 'w'),\n",
       " ('w', 'a'),\n",
       " ('a', 'l'),\n",
       " ('l', 'k'),\n",
       " ('k', ' '),\n",
       " (' ', 'w'),\n",
       " ('w', 'i'),\n",
       " ('i', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', ' '),\n",
       " (' ', 'K'),\n",
       " ('K', 'i'),\n",
       " ('i', 'n'),\n",
       " ('n', 'g'),\n",
       " ('g', 's'),\n",
       " ('s', '—'),\n",
       " ('—', 'n'),\n",
       " ('n', 'o'),\n",
       " ('o', 'r'),\n",
       " ('r', ' '),\n",
       " (' ', 'l'),\n",
       " ('l', 'o'),\n",
       " ('o', 's'),\n",
       " ('s', 'e'),\n",
       " ('e', ' '),\n",
       " (' ', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', 'e'),\n",
       " ('e', ' '),\n",
       " (' ', 'c'),\n",
       " ('c', 'o'),\n",
       " ('o', 'm'),\n",
       " ('m', 'm'),\n",
       " ('m', 'o'),\n",
       " ('o', 'n'),\n",
       " ('n', ' '),\n",
       " (' ', 't'),\n",
       " ('t', 'o'),\n",
       " ('o', 'u'),\n",
       " ('u', 'c'),\n",
       " ('c', 'h'),\n",
       " ('h', '.'),\n",
       " ('.', '\\n'),\n",
       " ('\\n', 'I'),\n",
       " ('I', 'f'),\n",
       " ('f', ' '),\n",
       " (' ', 'n'),\n",
       " ('n', 'e'),\n",
       " ('e', 'i'),\n",
       " ('i', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', 'e'),\n",
       " ('e', 'r'),\n",
       " ('r', ' '),\n",
       " (' ', 'f'),\n",
       " ('f', 'o'),\n",
       " ('o', 'e'),\n",
       " ('e', 's'),\n",
       " ('s', ' '),\n",
       " (' ', 'n'),\n",
       " ('n', 'o'),\n",
       " ('o', 'r'),\n",
       " ('r', ' '),\n",
       " (' ', 'l'),\n",
       " ('l', 'o'),\n",
       " ('o', 'v'),\n",
       " ('v', 'i'),\n",
       " ('i', 'n'),\n",
       " ('n', 'g'),\n",
       " ('g', ' '),\n",
       " (' ', 'f'),\n",
       " ('f', 'r'),\n",
       " ('r', 'i'),\n",
       " ('i', 'e'),\n",
       " ('e', 'n'),\n",
       " ('n', 'd'),\n",
       " ('d', 's'),\n",
       " ('s', ' '),\n",
       " (' ', 'c'),\n",
       " ('c', 'a'),\n",
       " ('a', 'n'),\n",
       " ('n', ' '),\n",
       " (' ', 'h'),\n",
       " ('h', 'u'),\n",
       " ('u', 'r'),\n",
       " ('r', 't'),\n",
       " ('t', ' '),\n",
       " (' ', 'y'),\n",
       " ('y', 'o'),\n",
       " ('o', 'u'),\n",
       " ('u', ','),\n",
       " (',', '\\n'),\n",
       " ('\\n', 'I'),\n",
       " ('I', 'f'),\n",
       " ('f', ' '),\n",
       " (' ', 'a'),\n",
       " ('a', 'l'),\n",
       " ('l', 'l'),\n",
       " ('l', ' '),\n",
       " (' ', 'm'),\n",
       " ('m', 'e'),\n",
       " ('e', 'n'),\n",
       " ('n', ' '),\n",
       " (' ', 'c'),\n",
       " ('c', 'o'),\n",
       " ('o', 'u'),\n",
       " ('u', 'n'),\n",
       " ('n', 't'),\n",
       " ('t', ' '),\n",
       " (' ', 'w'),\n",
       " ('w', 'i'),\n",
       " ('i', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', ' '),\n",
       " (' ', 'y'),\n",
       " ('y', 'o'),\n",
       " ('o', 'u'),\n",
       " ('u', ','),\n",
       " (',', ' '),\n",
       " (' ', 'b'),\n",
       " ('b', 'u'),\n",
       " ('u', 't'),\n",
       " ('t', ' '),\n",
       " (' ', 'n'),\n",
       " ('n', 'o'),\n",
       " ('o', 'n'),\n",
       " ('n', 'e'),\n",
       " ('e', ' '),\n",
       " (' ', 't'),\n",
       " ('t', 'o'),\n",
       " ('o', 'o'),\n",
       " ('o', ' '),\n",
       " (' ', 'm'),\n",
       " ('m', 'u'),\n",
       " ('u', 'c'),\n",
       " ('c', 'h'),\n",
       " ('h', '.'),\n",
       " ('.', '\\n'),\n",
       " ('\\n', 'I'),\n",
       " ('I', 'f'),\n",
       " ('f', ' '),\n",
       " (' ', 'y'),\n",
       " ('y', 'o'),\n",
       " ('o', 'u'),\n",
       " ('u', ' '),\n",
       " (' ', 'c'),\n",
       " ('c', 'a'),\n",
       " ('a', 'n'),\n",
       " ('n', ' '),\n",
       " (' ', 'f'),\n",
       " ('f', 'i'),\n",
       " ('i', 'l'),\n",
       " ('l', 'l'),\n",
       " ('l', ' '),\n",
       " (' ', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', 'e'),\n",
       " ('e', ' '),\n",
       " (' ', 'u'),\n",
       " ('u', 'n'),\n",
       " ('n', 'f'),\n",
       " ('f', 'o'),\n",
       " ('o', 'r'),\n",
       " ('r', 'g'),\n",
       " ('g', 'i'),\n",
       " ('i', 'v'),\n",
       " ('v', 'i'),\n",
       " ('i', 'n'),\n",
       " ('n', 'g'),\n",
       " ('g', ' '),\n",
       " (' ', 'm'),\n",
       " ('m', 'i'),\n",
       " ('i', 'n'),\n",
       " ('n', 'u'),\n",
       " ('u', 't'),\n",
       " ('t', 'e'),\n",
       " ('e', ','),\n",
       " (',', '\\n'),\n",
       " ('\\n', 'W'),\n",
       " ('W', 'i'),\n",
       " ('i', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', ' '),\n",
       " (' ', 's'),\n",
       " ('s', 'i'),\n",
       " ('i', 'x'),\n",
       " ('x', 't'),\n",
       " ('t', 'y'),\n",
       " ('y', ' '),\n",
       " (' ', 's'),\n",
       " ('s', 'e'),\n",
       " ('e', 'c'),\n",
       " ('c', 'o'),\n",
       " ('o', 'n'),\n",
       " ('n', 'd'),\n",
       " ('d', 's'),\n",
       " ('s', '’'),\n",
       " ('’', ' '),\n",
       " (' ', 'w'),\n",
       " ('w', 'o'),\n",
       " ('o', 'r'),\n",
       " ('r', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', ' '),\n",
       " (' ', 'o'),\n",
       " ('o', 'f'),\n",
       " ('f', ' '),\n",
       " (' ', 'd'),\n",
       " ('d', 'i'),\n",
       " ('i', 's'),\n",
       " ('s', 't'),\n",
       " ('t', 'a'),\n",
       " ('a', 'n'),\n",
       " ('n', 'c'),\n",
       " ('c', 'e'),\n",
       " ('e', ' '),\n",
       " (' ', 'r'),\n",
       " ('r', 'u'),\n",
       " ('u', 'n'),\n",
       " ('n', ','),\n",
       " (',', '\\n'),\n",
       " ('\\n', 'Y'),\n",
       " ('Y', 'o'),\n",
       " ('o', 'u'),\n",
       " ('u', 'r'),\n",
       " ('r', 's'),\n",
       " ('s', ' '),\n",
       " (' ', 'i'),\n",
       " ('i', 's'),\n",
       " ('s', ' '),\n",
       " (' ', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', 'e'),\n",
       " ('e', ' '),\n",
       " (' ', 'E'),\n",
       " ('E', 'a'),\n",
       " ('a', 'r'),\n",
       " ('r', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', ' '),\n",
       " (' ', 'a'),\n",
       " ('a', 'n'),\n",
       " ('n', 'd'),\n",
       " ('d', ' '),\n",
       " (' ', 'e'),\n",
       " ('e', 'v'),\n",
       " ('v', 'e'),\n",
       " ('e', 'r'),\n",
       " ('r', 'y'),\n",
       " ('y', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', 'i'),\n",
       " ('i', 'n'),\n",
       " ('n', 'g'),\n",
       " ('g', ' '),\n",
       " (' ', 't'),\n",
       " ('t', 'h'),\n",
       " ('h', 'a'),\n",
       " ('a', 't'),\n",
       " ('t', '’'),\n",
       " ('’', 's'),\n",
       " ('s', ' '),\n",
       " (' ', 'i'),\n",
       " ('i', 'n'),\n",
       " ('n', ' '),\n",
       " (' ', 'i'),\n",
       " ('i', 't'),\n",
       " ('t', ','),\n",
       " (',', '\\n'),\n",
       " ('\\n', 'A'),\n",
       " ('A', 'n'),\n",
       " ('n', 'd'),\n",
       " ('d', '—'),\n",
       " ('—', 'w'),\n",
       " ('w', 'h'),\n",
       " ('h', 'i'),\n",
       " ('i', 'c'),\n",
       " ('c', 'h'),\n",
       " ('h', ' '),\n",
       " (' ', 'i'),\n",
       " ('i', 's'),\n",
       " ('s', ' '),\n",
       " (' ', 'm'),\n",
       " ('m', 'o'),\n",
       " ('o', 'r'),\n",
       " ('r', 'e'),\n",
       " ('e', '—'),\n",
       " ('—', 'y'),\n",
       " ('y', 'o'),\n",
       " ('o', 'u'),\n",
       " ('u', '’'),\n",
       " ('’', 'l'),\n",
       " ('l', 'l'),\n",
       " ('l', ' '),\n",
       " (' ', 'b'),\n",
       " ('b', 'e'),\n",
       " ('e', ' '),\n",
       " (' ', 'a'),\n",
       " ('a', ' '),\n",
       " (' ', 'M'),\n",
       " ('M', 'a'),\n",
       " ('a', 'n'),\n",
       " ('n', ','),\n",
       " (',', ' '),\n",
       " (' ', 'm'),\n",
       " ('m', 'y'),\n",
       " ('y', ' '),\n",
       " (' ', 's'),\n",
       " ('s', 'o'),\n",
       " ('o', 'n'),\n",
       " ('n', '!')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'f', ' '),\n",
       " ('f', ' ', 'y'),\n",
       " (' ', 'y', 'o'),\n",
       " ('y', 'o', 'u'),\n",
       " ('o', 'u', ' '),\n",
       " ('u', ' ', 'c'),\n",
       " (' ', 'c', 'a'),\n",
       " ('c', 'a', 'n'),\n",
       " ('a', 'n', ' '),\n",
       " ('n', ' ', 't'),\n",
       " (' ', 't', 'a'),\n",
       " ('t', 'a', 'l'),\n",
       " ('a', 'l', 'k'),\n",
       " ('l', 'k', ' '),\n",
       " ('k', ' ', 'w'),\n",
       " (' ', 'w', 'i'),\n",
       " ('w', 'i', 't'),\n",
       " ('i', 't', 'h'),\n",
       " ('t', 'h', ' '),\n",
       " ('h', ' ', 'c'),\n",
       " (' ', 'c', 'r'),\n",
       " ('c', 'r', 'o'),\n",
       " ('r', 'o', 'w'),\n",
       " ('o', 'w', 'd'),\n",
       " ('w', 'd', 's'),\n",
       " ('d', 's', ' '),\n",
       " ('s', ' ', 'a'),\n",
       " (' ', 'a', 'n'),\n",
       " ('a', 'n', 'd'),\n",
       " ('n', 'd', ' '),\n",
       " ('d', ' ', 'k'),\n",
       " (' ', 'k', 'e'),\n",
       " ('k', 'e', 'e'),\n",
       " ('e', 'e', 'p'),\n",
       " ('e', 'p', ' '),\n",
       " ('p', ' ', 'y'),\n",
       " (' ', 'y', 'o'),\n",
       " ('y', 'o', 'u'),\n",
       " ('o', 'u', 'r'),\n",
       " ('u', 'r', ' '),\n",
       " ('r', ' ', 'v'),\n",
       " (' ', 'v', 'i'),\n",
       " ('v', 'i', 'r'),\n",
       " ('i', 'r', 't'),\n",
       " ('r', 't', 'u'),\n",
       " ('t', 'u', 'e'),\n",
       " ('u', 'e', ','),\n",
       " ('e', ',', '\\n'),\n",
       " (',', '\\n', 'O'),\n",
       " ('\\n', 'O', 'r'),\n",
       " ('O', 'r', ' '),\n",
       " ('r', ' ', 'w'),\n",
       " (' ', 'w', 'a'),\n",
       " ('w', 'a', 'l'),\n",
       " ('a', 'l', 'k'),\n",
       " ('l', 'k', ' '),\n",
       " ('k', ' ', 'w'),\n",
       " (' ', 'w', 'i'),\n",
       " ('w', 'i', 't'),\n",
       " ('i', 't', 'h'),\n",
       " ('t', 'h', ' '),\n",
       " ('h', ' ', 'K'),\n",
       " (' ', 'K', 'i'),\n",
       " ('K', 'i', 'n'),\n",
       " ('i', 'n', 'g'),\n",
       " ('n', 'g', 's'),\n",
       " ('g', 's', '—'),\n",
       " ('s', '—', 'n'),\n",
       " ('—', 'n', 'o'),\n",
       " ('n', 'o', 'r'),\n",
       " ('o', 'r', ' '),\n",
       " ('r', ' ', 'l'),\n",
       " (' ', 'l', 'o'),\n",
       " ('l', 'o', 's'),\n",
       " ('o', 's', 'e'),\n",
       " ('s', 'e', ' '),\n",
       " ('e', ' ', 't'),\n",
       " (' ', 't', 'h'),\n",
       " ('t', 'h', 'e'),\n",
       " ('h', 'e', ' '),\n",
       " ('e', ' ', 'c'),\n",
       " (' ', 'c', 'o'),\n",
       " ('c', 'o', 'm'),\n",
       " ('o', 'm', 'm'),\n",
       " ('m', 'm', 'o'),\n",
       " ('m', 'o', 'n'),\n",
       " ('o', 'n', ' '),\n",
       " ('n', ' ', 't'),\n",
       " (' ', 't', 'o'),\n",
       " ('t', 'o', 'u'),\n",
       " ('o', 'u', 'c'),\n",
       " ('u', 'c', 'h'),\n",
       " ('c', 'h', '.'),\n",
       " ('h', '.', '\\n'),\n",
       " ('.', '\\n', 'I'),\n",
       " ('\\n', 'I', 'f'),\n",
       " ('I', 'f', ' '),\n",
       " ('f', ' ', 'n'),\n",
       " (' ', 'n', 'e'),\n",
       " ('n', 'e', 'i'),\n",
       " ('e', 'i', 't'),\n",
       " ('i', 't', 'h'),\n",
       " ('t', 'h', 'e'),\n",
       " ('h', 'e', 'r'),\n",
       " ('e', 'r', ' '),\n",
       " ('r', ' ', 'f'),\n",
       " (' ', 'f', 'o'),\n",
       " ('f', 'o', 'e'),\n",
       " ('o', 'e', 's'),\n",
       " ('e', 's', ' '),\n",
       " ('s', ' ', 'n'),\n",
       " (' ', 'n', 'o'),\n",
       " ('n', 'o', 'r'),\n",
       " ('o', 'r', ' '),\n",
       " ('r', ' ', 'l'),\n",
       " (' ', 'l', 'o'),\n",
       " ('l', 'o', 'v'),\n",
       " ('o', 'v', 'i'),\n",
       " ('v', 'i', 'n'),\n",
       " ('i', 'n', 'g'),\n",
       " ('n', 'g', ' '),\n",
       " ('g', ' ', 'f'),\n",
       " (' ', 'f', 'r'),\n",
       " ('f', 'r', 'i'),\n",
       " ('r', 'i', 'e'),\n",
       " ('i', 'e', 'n'),\n",
       " ('e', 'n', 'd'),\n",
       " ('n', 'd', 's'),\n",
       " ('d', 's', ' '),\n",
       " ('s', ' ', 'c'),\n",
       " (' ', 'c', 'a'),\n",
       " ('c', 'a', 'n'),\n",
       " ('a', 'n', ' '),\n",
       " ('n', ' ', 'h'),\n",
       " (' ', 'h', 'u'),\n",
       " ('h', 'u', 'r'),\n",
       " ('u', 'r', 't'),\n",
       " ('r', 't', ' '),\n",
       " ('t', ' ', 'y'),\n",
       " (' ', 'y', 'o'),\n",
       " ('y', 'o', 'u'),\n",
       " ('o', 'u', ','),\n",
       " ('u', ',', '\\n'),\n",
       " (',', '\\n', 'I'),\n",
       " ('\\n', 'I', 'f'),\n",
       " ('I', 'f', ' '),\n",
       " ('f', ' ', 'a'),\n",
       " (' ', 'a', 'l'),\n",
       " ('a', 'l', 'l'),\n",
       " ('l', 'l', ' '),\n",
       " ('l', ' ', 'm'),\n",
       " (' ', 'm', 'e'),\n",
       " ('m', 'e', 'n'),\n",
       " ('e', 'n', ' '),\n",
       " ('n', ' ', 'c'),\n",
       " (' ', 'c', 'o'),\n",
       " ('c', 'o', 'u'),\n",
       " ('o', 'u', 'n'),\n",
       " ('u', 'n', 't'),\n",
       " ('n', 't', ' '),\n",
       " ('t', ' ', 'w'),\n",
       " (' ', 'w', 'i'),\n",
       " ('w', 'i', 't'),\n",
       " ('i', 't', 'h'),\n",
       " ('t', 'h', ' '),\n",
       " ('h', ' ', 'y'),\n",
       " (' ', 'y', 'o'),\n",
       " ('y', 'o', 'u'),\n",
       " ('o', 'u', ','),\n",
       " ('u', ',', ' '),\n",
       " (',', ' ', 'b'),\n",
       " (' ', 'b', 'u'),\n",
       " ('b', 'u', 't'),\n",
       " ('u', 't', ' '),\n",
       " ('t', ' ', 'n'),\n",
       " (' ', 'n', 'o'),\n",
       " ('n', 'o', 'n'),\n",
       " ('o', 'n', 'e'),\n",
       " ('n', 'e', ' '),\n",
       " ('e', ' ', 't'),\n",
       " (' ', 't', 'o'),\n",
       " ('t', 'o', 'o'),\n",
       " ('o', 'o', ' '),\n",
       " ('o', ' ', 'm'),\n",
       " (' ', 'm', 'u'),\n",
       " ('m', 'u', 'c'),\n",
       " ('u', 'c', 'h'),\n",
       " ('c', 'h', '.'),\n",
       " ('h', '.', '\\n'),\n",
       " ('.', '\\n', 'I'),\n",
       " ('\\n', 'I', 'f'),\n",
       " ('I', 'f', ' '),\n",
       " ('f', ' ', 'y'),\n",
       " (' ', 'y', 'o'),\n",
       " ('y', 'o', 'u'),\n",
       " ('o', 'u', ' '),\n",
       " ('u', ' ', 'c'),\n",
       " (' ', 'c', 'a'),\n",
       " ('c', 'a', 'n'),\n",
       " ('a', 'n', ' '),\n",
       " ('n', ' ', 'f'),\n",
       " (' ', 'f', 'i'),\n",
       " ('f', 'i', 'l'),\n",
       " ('i', 'l', 'l'),\n",
       " ('l', 'l', ' '),\n",
       " ('l', ' ', 't'),\n",
       " (' ', 't', 'h'),\n",
       " ('t', 'h', 'e'),\n",
       " ('h', 'e', ' '),\n",
       " ('e', ' ', 'u'),\n",
       " (' ', 'u', 'n'),\n",
       " ('u', 'n', 'f'),\n",
       " ('n', 'f', 'o'),\n",
       " ('f', 'o', 'r'),\n",
       " ('o', 'r', 'g'),\n",
       " ('r', 'g', 'i'),\n",
       " ('g', 'i', 'v'),\n",
       " ('i', 'v', 'i'),\n",
       " ('v', 'i', 'n'),\n",
       " ('i', 'n', 'g'),\n",
       " ('n', 'g', ' '),\n",
       " ('g', ' ', 'm'),\n",
       " (' ', 'm', 'i'),\n",
       " ('m', 'i', 'n'),\n",
       " ('i', 'n', 'u'),\n",
       " ('n', 'u', 't'),\n",
       " ('u', 't', 'e'),\n",
       " ('t', 'e', ','),\n",
       " ('e', ',', '\\n'),\n",
       " (',', '\\n', 'W'),\n",
       " ('\\n', 'W', 'i'),\n",
       " ('W', 'i', 't'),\n",
       " ('i', 't', 'h'),\n",
       " ('t', 'h', ' '),\n",
       " ('h', ' ', 's'),\n",
       " (' ', 's', 'i'),\n",
       " ('s', 'i', 'x'),\n",
       " ('i', 'x', 't'),\n",
       " ('x', 't', 'y'),\n",
       " ('t', 'y', ' '),\n",
       " ('y', ' ', 's'),\n",
       " (' ', 's', 'e'),\n",
       " ('s', 'e', 'c'),\n",
       " ('e', 'c', 'o'),\n",
       " ('c', 'o', 'n'),\n",
       " ('o', 'n', 'd'),\n",
       " ('n', 'd', 's'),\n",
       " ('d', 's', '’'),\n",
       " ('s', '’', ' '),\n",
       " ('’', ' ', 'w'),\n",
       " (' ', 'w', 'o'),\n",
       " ('w', 'o', 'r'),\n",
       " ('o', 'r', 't'),\n",
       " ('r', 't', 'h'),\n",
       " ('t', 'h', ' '),\n",
       " ('h', ' ', 'o'),\n",
       " (' ', 'o', 'f'),\n",
       " ('o', 'f', ' '),\n",
       " ('f', ' ', 'd'),\n",
       " (' ', 'd', 'i'),\n",
       " ('d', 'i', 's'),\n",
       " ('i', 's', 't'),\n",
       " ('s', 't', 'a'),\n",
       " ('t', 'a', 'n'),\n",
       " ('a', 'n', 'c'),\n",
       " ('n', 'c', 'e'),\n",
       " ('c', 'e', ' '),\n",
       " ('e', ' ', 'r'),\n",
       " (' ', 'r', 'u'),\n",
       " ('r', 'u', 'n'),\n",
       " ('u', 'n', ','),\n",
       " ('n', ',', '\\n'),\n",
       " (',', '\\n', 'Y'),\n",
       " ('\\n', 'Y', 'o'),\n",
       " ('Y', 'o', 'u'),\n",
       " ('o', 'u', 'r'),\n",
       " ('u', 'r', 's'),\n",
       " ('r', 's', ' '),\n",
       " ('s', ' ', 'i'),\n",
       " (' ', 'i', 's'),\n",
       " ('i', 's', ' '),\n",
       " ('s', ' ', 't'),\n",
       " (' ', 't', 'h'),\n",
       " ('t', 'h', 'e'),\n",
       " ('h', 'e', ' '),\n",
       " ('e', ' ', 'E'),\n",
       " (' ', 'E', 'a'),\n",
       " ('E', 'a', 'r'),\n",
       " ('a', 'r', 't'),\n",
       " ('r', 't', 'h'),\n",
       " ('t', 'h', ' '),\n",
       " ('h', ' ', 'a'),\n",
       " (' ', 'a', 'n'),\n",
       " ('a', 'n', 'd'),\n",
       " ('n', 'd', ' '),\n",
       " ('d', ' ', 'e'),\n",
       " (' ', 'e', 'v'),\n",
       " ('e', 'v', 'e'),\n",
       " ('v', 'e', 'r'),\n",
       " ('e', 'r', 'y'),\n",
       " ('r', 'y', 't'),\n",
       " ('y', 't', 'h'),\n",
       " ('t', 'h', 'i'),\n",
       " ('h', 'i', 'n'),\n",
       " ('i', 'n', 'g'),\n",
       " ('n', 'g', ' '),\n",
       " ('g', ' ', 't'),\n",
       " (' ', 't', 'h'),\n",
       " ('t', 'h', 'a'),\n",
       " ('h', 'a', 't'),\n",
       " ('a', 't', '’'),\n",
       " ('t', '’', 's'),\n",
       " ('’', 's', ' '),\n",
       " ('s', ' ', 'i'),\n",
       " (' ', 'i', 'n'),\n",
       " ('i', 'n', ' '),\n",
       " ('n', ' ', 'i'),\n",
       " (' ', 'i', 't'),\n",
       " ('i', 't', ','),\n",
       " ('t', ',', '\\n'),\n",
       " (',', '\\n', 'A'),\n",
       " ('\\n', 'A', 'n'),\n",
       " ('A', 'n', 'd'),\n",
       " ('n', 'd', '—'),\n",
       " ('d', '—', 'w'),\n",
       " ('—', 'w', 'h'),\n",
       " ('w', 'h', 'i'),\n",
       " ('h', 'i', 'c'),\n",
       " ('i', 'c', 'h'),\n",
       " ('c', 'h', ' '),\n",
       " ('h', ' ', 'i'),\n",
       " (' ', 'i', 's'),\n",
       " ('i', 's', ' '),\n",
       " ('s', ' ', 'm'),\n",
       " (' ', 'm', 'o'),\n",
       " ('m', 'o', 'r'),\n",
       " ('o', 'r', 'e'),\n",
       " ('r', 'e', '—'),\n",
       " ('e', '—', 'y'),\n",
       " ('—', 'y', 'o'),\n",
       " ('y', 'o', 'u'),\n",
       " ('o', 'u', '’'),\n",
       " ('u', '’', 'l'),\n",
       " ('’', 'l', 'l'),\n",
       " ('l', 'l', ' '),\n",
       " ('l', ' ', 'b'),\n",
       " (' ', 'b', 'e'),\n",
       " ('b', 'e', ' '),\n",
       " ('e', ' ', 'a'),\n",
       " (' ', 'a', ' '),\n",
       " ('a', ' ', 'M'),\n",
       " (' ', 'M', 'a'),\n",
       " ('M', 'a', 'n'),\n",
       " ('a', 'n', ','),\n",
       " ('n', ',', ' '),\n",
       " (',', ' ', 'm'),\n",
       " (' ', 'm', 'y'),\n",
       " ('m', 'y', ' '),\n",
       " ('y', ' ', 's'),\n",
       " (' ', 's', 'o'),\n",
       " ('s', 'o', 'n'),\n",
       " ('o', 'n', '!')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include the n-grams as a string rather than as tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I f', 'f  ', '  y', 'y o', 'o u', 'u  ', '  c', 'c a', 'a n', 'n  ', '  t', 't a', 'a l', 'l k', 'k  ', '  w', 'w i', 'i t', 't h', 'h  ', '  c', 'c r', 'r o', 'o w', 'w d', 'd s', 's  ', '  a', 'a n', 'n d', 'd  ', '  k', 'k e', 'e e', 'e p', 'p  ', '  y', 'y o', 'o u', 'u r', 'r  ', '  v', 'v i', 'i r', 'r t', 't u', 'u e', 'e ,', ', \\n', '\\n O', 'O r', 'r  ', '  w', 'w a', 'a l', 'l k', 'k  ', '  w', 'w i', 'i t', 't h', 'h  ', '  K', 'K i', 'i n', 'n g', 'g s', 's —', '— n', 'n o', 'o r', 'r  ', '  l', 'l o', 'o s', 's e', 'e  ', '  t', 't h', 'h e', 'e  ', '  c', 'c o', 'o m', 'm m', 'm o', 'o n', 'n  ', '  t', 't o', 'o u', 'u c', 'c h', 'h .', '. \\n', '\\n I', 'I f', 'f  ', '  n', 'n e', 'e i', 'i t', 't h', 'h e', 'e r', 'r  ', '  f', 'f o', 'o e', 'e s', 's  ', '  n', 'n o', 'o r', 'r  ', '  l', 'l o', 'o v', 'v i', 'i n', 'n g', 'g  ', '  f', 'f r', 'r i', 'i e', 'e n', 'n d', 'd s', 's  ', '  c', 'c a', 'a n', 'n  ', '  h', 'h u', 'u r', 'r t', 't  ', '  y', 'y o', 'o u', 'u ,', ', \\n', '\\n I', 'I f', 'f  ', '  a', 'a l', 'l l', 'l  ', '  m', 'm e', 'e n', 'n  ', '  c', 'c o', 'o u', 'u n', 'n t', 't  ', '  w', 'w i', 'i t', 't h', 'h  ', '  y', 'y o', 'o u', 'u ,', ',  ', '  b', 'b u', 'u t', 't  ', '  n', 'n o', 'o n', 'n e', 'e  ', '  t', 't o', 'o o', 'o  ', '  m', 'm u', 'u c', 'c h', 'h .', '. \\n', '\\n I', 'I f', 'f  ', '  y', 'y o', 'o u', 'u  ', '  c', 'c a', 'a n', 'n  ', '  f', 'f i', 'i l', 'l l', 'l  ', '  t', 't h', 'h e', 'e  ', '  u', 'u n', 'n f', 'f o', 'o r', 'r g', 'g i', 'i v', 'v i', 'i n', 'n g', 'g  ', '  m', 'm i', 'i n', 'n u', 'u t', 't e', 'e ,', ', \\n', '\\n W', 'W i', 'i t', 't h', 'h  ', '  s', 's i', 'i x', 'x t', 't y', 'y  ', '  s', 's e', 'e c', 'c o', 'o n', 'n d', 'd s', 's ’', '’  ', '  w', 'w o', 'o r', 'r t', 't h', 'h  ', '  o', 'o f', 'f  ', '  d', 'd i', 'i s', 's t', 't a', 'a n', 'n c', 'c e', 'e  ', '  r', 'r u', 'u n', 'n ,', ', \\n', '\\n Y', 'Y o', 'o u', 'u r', 'r s', 's  ', '  i', 'i s', 's  ', '  t', 't h', 'h e', 'e  ', '  E', 'E a', 'a r', 'r t', 't h', 'h  ', '  a', 'a n', 'n d', 'd  ', '  e', 'e v', 'v e', 'e r', 'r y', 'y t', 't h', 'h i', 'i n', 'n g', 'g  ', '  t', 't h', 'h a', 'a t', 't ’', '’ s', 's  ', '  i', 'i n', 'n  ', '  i', 'i t', 't ,', ', \\n', '\\n A', 'A n', 'n d', 'd —', '— w', 'w h', 'h i', 'i c', 'c h', 'h  ', '  i', 'i s', 's  ', '  m', 'm o', 'o r', 'r e', 'e —', '— y', 'y o', 'o u', 'u ’', '’ l', 'l l', 'l  ', '  b', 'b e', 'e  ', '  a', 'a  ', '  M', 'M a', 'a n', 'n ,', ',  ', '  m', 'm y', 'y  ', '  s', 's o', 'o n', 'n !']\n",
      "\n",
      "\n",
      "\n",
      "['I f  ', 'f   y', '  y o', 'y o u', 'o u  ', 'u   c', '  c a', 'c a n', 'a n  ', 'n   t', '  t a', 't a l', 'a l k', 'l k  ', 'k   w', '  w i', 'w i t', 'i t h', 't h  ', 'h   c', '  c r', 'c r o', 'r o w', 'o w d', 'w d s', 'd s  ', 's   a', '  a n', 'a n d', 'n d  ', 'd   k', '  k e', 'k e e', 'e e p', 'e p  ', 'p   y', '  y o', 'y o u', 'o u r', 'u r  ', 'r   v', '  v i', 'v i r', 'i r t', 'r t u', 't u e', 'u e ,', 'e , \\n', ', \\n O', '\\n O r', 'O r  ', 'r   w', '  w a', 'w a l', 'a l k', 'l k  ', 'k   w', '  w i', 'w i t', 'i t h', 't h  ', 'h   K', '  K i', 'K i n', 'i n g', 'n g s', 'g s —', 's — n', '— n o', 'n o r', 'o r  ', 'r   l', '  l o', 'l o s', 'o s e', 's e  ', 'e   t', '  t h', 't h e', 'h e  ', 'e   c', '  c o', 'c o m', 'o m m', 'm m o', 'm o n', 'o n  ', 'n   t', '  t o', 't o u', 'o u c', 'u c h', 'c h .', 'h . \\n', '. \\n I', '\\n I f', 'I f  ', 'f   n', '  n e', 'n e i', 'e i t', 'i t h', 't h e', 'h e r', 'e r  ', 'r   f', '  f o', 'f o e', 'o e s', 'e s  ', 's   n', '  n o', 'n o r', 'o r  ', 'r   l', '  l o', 'l o v', 'o v i', 'v i n', 'i n g', 'n g  ', 'g   f', '  f r', 'f r i', 'r i e', 'i e n', 'e n d', 'n d s', 'd s  ', 's   c', '  c a', 'c a n', 'a n  ', 'n   h', '  h u', 'h u r', 'u r t', 'r t  ', 't   y', '  y o', 'y o u', 'o u ,', 'u , \\n', ', \\n I', '\\n I f', 'I f  ', 'f   a', '  a l', 'a l l', 'l l  ', 'l   m', '  m e', 'm e n', 'e n  ', 'n   c', '  c o', 'c o u', 'o u n', 'u n t', 'n t  ', 't   w', '  w i', 'w i t', 'i t h', 't h  ', 'h   y', '  y o', 'y o u', 'o u ,', 'u ,  ', ',   b', '  b u', 'b u t', 'u t  ', 't   n', '  n o', 'n o n', 'o n e', 'n e  ', 'e   t', '  t o', 't o o', 'o o  ', 'o   m', '  m u', 'm u c', 'u c h', 'c h .', 'h . \\n', '. \\n I', '\\n I f', 'I f  ', 'f   y', '  y o', 'y o u', 'o u  ', 'u   c', '  c a', 'c a n', 'a n  ', 'n   f', '  f i', 'f i l', 'i l l', 'l l  ', 'l   t', '  t h', 't h e', 'h e  ', 'e   u', '  u n', 'u n f', 'n f o', 'f o r', 'o r g', 'r g i', 'g i v', 'i v i', 'v i n', 'i n g', 'n g  ', 'g   m', '  m i', 'm i n', 'i n u', 'n u t', 'u t e', 't e ,', 'e , \\n', ', \\n W', '\\n W i', 'W i t', 'i t h', 't h  ', 'h   s', '  s i', 's i x', 'i x t', 'x t y', 't y  ', 'y   s', '  s e', 's e c', 'e c o', 'c o n', 'o n d', 'n d s', 'd s ’', 's ’  ', '’   w', '  w o', 'w o r', 'o r t', 'r t h', 't h  ', 'h   o', '  o f', 'o f  ', 'f   d', '  d i', 'd i s', 'i s t', 's t a', 't a n', 'a n c', 'n c e', 'c e  ', 'e   r', '  r u', 'r u n', 'u n ,', 'n , \\n', ', \\n Y', '\\n Y o', 'Y o u', 'o u r', 'u r s', 'r s  ', 's   i', '  i s', 'i s  ', 's   t', '  t h', 't h e', 'h e  ', 'e   E', '  E a', 'E a r', 'a r t', 'r t h', 't h  ', 'h   a', '  a n', 'a n d', 'n d  ', 'd   e', '  e v', 'e v e', 'v e r', 'e r y', 'r y t', 'y t h', 't h i', 'h i n', 'i n g', 'n g  ', 'g   t', '  t h', 't h a', 'h a t', 'a t ’', 't ’ s', '’ s  ', 's   i', '  i n', 'i n  ', 'n   i', '  i t', 'i t ,', 't , \\n', ', \\n A', '\\n A n', 'A n d', 'n d —', 'd — w', '— w h', 'w h i', 'h i c', 'i c h', 'c h  ', 'h   i', '  i s', 'i s  ', 's   m', '  m o', 'm o r', 'o r e', 'r e —', 'e — y', '— y o', 'y o u', 'o u ’', 'u ’ l', '’ l l', 'l l  ', 'l   b', '  b e', 'b e  ', 'e   a', '  a  ', 'a   M', '  M a', 'M a n', 'a n ,', 'n ,  ', ',   m', '  m y', 'm y  ', 'y   s', '  s o', 's o n', 'o n !']\n"
     ]
    }
   ],
   "source": [
    "bigrams = [\" \".join(x) for x in list(ngrams(tokens, 2))]\n",
    "print(bigrams)\n",
    "trigrams = [\" \".join(x) for x in list(ngrams(tokens, 3))]\n",
    "print(\"\\n\\n\")\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's download the list from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nn007/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then check it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 179\n",
      "\n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(\"number of stopwords:\", len(stop_words))\n",
    "print(\"\\n\",stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other libs have different stopwords. Let's see a much larger set from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stopwords: 318\n",
      "\n",
      " frozenset({'must', 'each', 'made', 'something', 'had', 'get', 'there', 'at', 'behind', 'whereafter', 'three', 'couldnt', 'twelve', 'whereby', 'fifty', 'sometime', 'another', 'because', 'became', 'this', 'over', 'once', 'system', 'per', 'amoungst', 'often', 'throughout', 'one', 'not', 'almost', 'yourselves', 'several', 'their', 'thin', 'has', 'same', 'after', 'nine', 'rather', 'all', 'top', 'interest', 'part', 'less', 'empty', 'via', 'without', 'anyhow', 'our', 'done', 'some', 'whose', 'except', 'thereby', 'always', 'amongst', 'upon', 'be', 'very', 'take', 'beside', 'these', 'thus', 'until', 'wherein', 'more', 'any', 'for', 'ltd', 'cannot', 'ie', 'however', 'across', 'whom', 'hereafter', 'them', 'un', 'seems', 'the', 'others', 'such', 'fill', 'even', 'it', 'or', 'anyone', 'someone', 'call', 'down', 'none', 'us', 'never', 'i', 'ten', 'here', 'but', 'you', 'her', 'me', 'hence', 'both', 'twenty', 'mill', 'whence', 'either', 'other', 'sixty', 'your', 'will', 'him', 'up', 'he', 'please', 'off', 'sometimes', 'further', 'seem', 'they', 'found', 'thru', 'while', 'side', 'yours', 'then', 'can', 'co', 'am', 'eleven', 'my', 'against', 'around', 'former', 'becomes', 'go', 'myself', 'nevertheless', 'ever', 'describe', 'towards', 'those', 'before', 'being', 'give', 'formerly', 'hereupon', 'forty', 'whenever', 'itself', 'could', 'con', 'beyond', 'elsewhere', 'therein', 'was', 'only', 'wherever', 'latter', 'whatever', 'six', 'might', 'becoming', 'noone', 'too', 'few', 'were', 'much', 'anywhere', 'below', 'where', 'back', 'two', 'see', 'next', 'front', 'somewhere', 'nowhere', 'name', 'nobody', 'are', 'its', 'been', 'hasnt', 'thereupon', 'bottom', 'how', 'due', 'when', 'indeed', 'who', 'everything', 'from', 'hereby', 'to', 'every', 'during', 'least', 'have', 'as', 'eight', 'now', 'under', 'no', 'what', 'do', 'among', 'serious', 'anything', 'although', 'ourselves', 'otherwise', 'full', 'everyone', 'detail', 'though', 'de', 'last', 'somehow', 'hundred', 'may', 'we', 'herself', 'own', 'seemed', 'whole', 'everywhere', 'on', 'fifteen', 'thick', 'namely', 'moreover', 'amount', 'is', 'themselves', 'of', 'fire', 'become', 'ours', 'also', 'anyway', 'above', 're', 'a', 'together', 'yourself', 'she', 'first', 'about', 'most', 'many', 'whoever', 'along', 'thence', 'would', 'enough', 'again', 'with', 'out', 'cant', 'nothing', 'latterly', 'thereafter', 'alone', 'why', 'etc', 'move', 'mostly', 'whereupon', 'whereas', 'through', 'within', 'toward', 'onto', 'and', 'find', 'mine', 'herein', 'hers', 'keep', 'show', 'in', 'already', 'which', 'by', 'if', 'nor', 'beforehand', 'four', 'his', 'eg', 'meanwhile', 'that', 'besides', 'perhaps', 'an', 'cry', 'whether', 'neither', 'whither', 'put', 'therefore', 'afterwards', 'so', 'else', 'should', 'well', 'into', 'third', 'seeming', 'still', 'between', 'than', 'inc', 'sincere', 'bill', 'since', 'five', 'himself', 'yet'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "\n",
    "print(\"number of stopwords:\", len(sklearn_stop_words))\n",
    "print(\"\\n\",sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that although there are more stopwords in sklearn, nltk has words that are not contained in sklearn. So we might want to join the two lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normalizing the text we could do something as simple as making sure all words are lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 't', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'c', 'r', 'o', 'w', 'd', 's', ' ', 'a', 'n', 'd', ' ', 'k', 'e', 'e', 'p', ' ', 'y', 'o', 'u', 'r', ' ', 'v', 'i', 'r', 't', 'u', 'e', ',', '\\n', 'o', 'r', ' ', 'w', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'k', 'i', 'n', 'g', 's', '—', 'n', 'o', 'r', ' ', 'l', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'm', 'm', 'o', 'n', ' ', 't', 'o', 'u', 'c', 'h', '.', '\\n', 'i', 'f', ' ', 'n', 'e', 'i', 't', 'h', 'e', 'r', ' ', 'f', 'o', 'e', 's', ' ', 'n', 'o', 'r', ' ', 'l', 'o', 'v', 'i', 'n', 'g', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 's', ' ', 'c', 'a', 'n', ' ', 'h', 'u', 'r', 't', ' ', 'y', 'o', 'u', ',', '\\n', 'i', 'f', ' ', 'a', 'l', 'l', ' ', 'm', 'e', 'n', ' ', 'c', 'o', 'u', 'n', 't', ' ', 'w', 'i', 't', 'h', ' ', 'y', 'o', 'u', ',', ' ', 'b', 'u', 't', ' ', 'n', 'o', 'n', 'e', ' ', 't', 'o', 'o', ' ', 'm', 'u', 'c', 'h', '.', '\\n', 'i', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'f', 'i', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'u', 'n', 'f', 'o', 'r', 'g', 'i', 'v', 'i', 'n', 'g', ' ', 'm', 'i', 'n', 'u', 't', 'e', ',', '\\n', 'w', 'i', 't', 'h', ' ', 's', 'i', 'x', 't', 'y', ' ', 's', 'e', 'c', 'o', 'n', 'd', 's', '’', ' ', 'w', 'o', 'r', 't', 'h', ' ', 'o', 'f', ' ', 'd', 'i', 's', 't', 'a', 'n', 'c', 'e', ' ', 'r', 'u', 'n', ',', '\\n', 'y', 'o', 'u', 'r', 's', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'e', 'a', 'r', 't', 'h', ' ', 'a', 'n', 'd', ' ', 'e', 'v', 'e', 'r', 'y', 't', 'h', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', '’', 's', ' ', 'i', 'n', ' ', 'i', 't', ',', '\\n', 'a', 'n', 'd', '—', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 's', ' ', 'm', 'o', 'r', 'e', '—', 'y', 'o', 'u', '’', 'l', 'l', ' ', 'b', 'e', ' ', 'a', ' ', 'm', 'a', 'n', ',', ' ', 'm', 'y', ' ', 's', 'o', 'n', '!']\n"
     ]
    }
   ],
   "source": [
    "norm_tokens = [x.lower() for x in tokens]\n",
    "print(norm_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For stemming the words, we can use NLTK again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 't', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'c', 'r', 'o', 'w', 'd', 's', ' ', 'a', 'n', 'd', ' ', 'k', 'e', 'e', 'p', ' ', 'y', 'o', 'u', 'r', ' ', 'v', 'i', 'r', 't', 'u', 'e', ',', '\\n', 'o', 'r', ' ', 'w', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'k', 'i', 'n', 'g', 's', '—', 'n', 'o', 'r', ' ', 'l', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'm', 'm', 'o', 'n', ' ', 't', 'o', 'u', 'c', 'h', '.', '\\n', 'i', 'f', ' ', 'n', 'e', 'i', 't', 'h', 'e', 'r', ' ', 'f', 'o', 'e', 's', ' ', 'n', 'o', 'r', ' ', 'l', 'o', 'v', 'i', 'n', 'g', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 's', ' ', 'c', 'a', 'n', ' ', 'h', 'u', 'r', 't', ' ', 'y', 'o', 'u', ',', '\\n', 'i', 'f', ' ', 'a', 'l', 'l', ' ', 'm', 'e', 'n', ' ', 'c', 'o', 'u', 'n', 't', ' ', 'w', 'i', 't', 'h', ' ', 'y', 'o', 'u', ',', ' ', 'b', 'u', 't', ' ', 'n', 'o', 'n', 'e', ' ', 't', 'o', 'o', ' ', 'm', 'u', 'c', 'h', '.', '\\n', 'i', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'f', 'i', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'u', 'n', 'f', 'o', 'r', 'g', 'i', 'v', 'i', 'n', 'g', ' ', 'm', 'i', 'n', 'u', 't', 'e', ',', '\\n', 'w', 'i', 't', 'h', ' ', 's', 'i', 'x', 't', 'y', ' ', 's', 'e', 'c', 'o', 'n', 'd', 's', '’', ' ', 'w', 'o', 'r', 't', 'h', ' ', 'o', 'f', ' ', 'd', 'i', 's', 't', 'a', 'n', 'c', 'e', ' ', 'r', 'u', 'n', ',', '\\n', 'y', 'o', 'u', 'r', 's', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'e', 'a', 'r', 't', 'h', ' ', 'a', 'n', 'd', ' ', 'e', 'v', 'e', 'r', 'y', 't', 'h', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', '’', 's', ' ', 'i', 'n', ' ', 'i', 't', ',', '\\n', 'a', 'n', 'd', '—', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 's', ' ', 'm', 'o', 'r', 'e', '—', 'y', 'o', 'u', '’', 'l', 'l', ' ', 'b', 'e', ' ', 'a', ' ', 'm', 'a', 'n', ',', ' ', 'm', 'y', ' ', 's', 'o', 'n', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stem_tokens = [stemmer.stem(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lemmatising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 't', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'c', 'r', 'o', 'w', 'd', 's', ' ', 'a', 'n', 'd', ' ', 'k', 'e', 'e', 'p', ' ', 'y', 'o', 'u', 'r', ' ', 'v', 'i', 'r', 't', 'u', 'e', ',', '\\n', 'o', 'r', ' ', 'w', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'k', 'i', 'n', 'g', 's', '—', 'n', 'o', 'r', ' ', 'l', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'm', 'm', 'o', 'n', ' ', 't', 'o', 'u', 'c', 'h', '.', '\\n', 'i', 'f', ' ', 'n', 'e', 'i', 't', 'h', 'e', 'r', ' ', 'f', 'o', 'e', 's', ' ', 'n', 'o', 'r', ' ', 'l', 'o', 'v', 'i', 'n', 'g', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 's', ' ', 'c', 'a', 'n', ' ', 'h', 'u', 'r', 't', ' ', 'y', 'o', 'u', ',', '\\n', 'i', 'f', ' ', 'a', 'l', 'l', ' ', 'm', 'e', 'n', ' ', 'c', 'o', 'u', 'n', 't', ' ', 'w', 'i', 't', 'h', ' ', 'y', 'o', 'u', ',', ' ', 'b', 'u', 't', ' ', 'n', 'o', 'n', 'e', ' ', 't', 'o', 'o', ' ', 'm', 'u', 'c', 'h', '.', '\\n', 'i', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'f', 'i', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'u', 'n', 'f', 'o', 'r', 'g', 'i', 'v', 'i', 'n', 'g', ' ', 'm', 'i', 'n', 'u', 't', 'e', ',', '\\n', 'w', 'i', 't', 'h', ' ', 's', 'i', 'x', 't', 'y', ' ', 's', 'e', 'c', 'o', 'n', 'd', 's', '’', ' ', 'w', 'o', 'r', 't', 'h', ' ', 'o', 'f', ' ', 'd', 'i', 's', 't', 'a', 'n', 'c', 'e', ' ', 'r', 'u', 'n', ',', '\\n', 'y', 'o', 'u', 'r', 's', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'e', 'a', 'r', 't', 'h', ' ', 'a', 'n', 'd', ' ', 'e', 'v', 'e', 'r', 'y', 't', 'h', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', '’', 's', ' ', 'i', 'n', ' ', 'i', 't', ',', '\\n', 'a', 'n', 'd', '—', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 's', ' ', 'm', 'o', 'r', 'e', '—', 'y', 'o', 'u', '’', 'l', 'l', ' ', 'b', 'e', ' ', 'a', ' ', 'm', 'a', 'n', ',', ' ', 'm', 'y', ' ', 's', 'o', 'n', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nn007/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stem_tokens = [lemmatizer.lemmatize(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences we have has no issues with the lemma... but look into the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", 'a')) # declaring the POS as adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't include the POS, the nltk library with wordnet does not work well. So let's try fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nn007/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    # now we need to convert from nltk to wordnet POS notations (for compatibility reasons)\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # return and default to noun if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 't', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'c', 'r', 'o', 'w', 'd', 's', ' ', 'a', 'n', 'd', ' ', 'k', 'e', 'e', 'p', ' ', 'y', 'o', 'u', 'r', ' ', 'v', 'i', 'r', 't', 'u', 'e', ',', '\\n', 'o', 'r', ' ', 'w', 'a', 'l', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'k', 'i', 'n', 'g', 's', '—', 'n', 'o', 'r', ' ', 'l', 'o', 's', 'e', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'm', 'm', 'o', 'n', ' ', 't', 'o', 'u', 'c', 'h', '.', '\\n', 'i', 'f', ' ', 'n', 'e', 'i', 't', 'h', 'e', 'r', ' ', 'f', 'o', 'e', 's', ' ', 'n', 'o', 'r', ' ', 'l', 'o', 'v', 'i', 'n', 'g', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 's', ' ', 'c', 'a', 'n', ' ', 'h', 'u', 'r', 't', ' ', 'y', 'o', 'u', ',', '\\n', 'i', 'f', ' ', 'a', 'l', 'l', ' ', 'm', 'e', 'n', ' ', 'c', 'o', 'u', 'n', 't', ' ', 'w', 'i', 't', 'h', ' ', 'y', 'o', 'u', ',', ' ', 'b', 'u', 't', ' ', 'n', 'o', 'n', 'e', ' ', 't', 'o', 'o', ' ', 'm', 'u', 'c', 'h', '.', '\\n', 'i', 'f', ' ', 'y', 'o', 'u', ' ', 'c', 'a', 'n', ' ', 'f', 'i', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'u', 'n', 'f', 'o', 'r', 'g', 'i', 'v', 'i', 'n', 'g', ' ', 'm', 'i', 'n', 'u', 't', 'e', ',', '\\n', 'w', 'i', 't', 'h', ' ', 's', 'i', 'x', 't', 'y', ' ', 's', 'e', 'c', 'o', 'n', 'd', 's', '’', ' ', 'w', 'o', 'r', 't', 'h', ' ', 'o', 'f', ' ', 'd', 'i', 's', 't', 'a', 'n', 'c', 'e', ' ', 'r', 'u', 'n', ',', '\\n', 'y', 'o', 'u', 'r', 's', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'e', 'a', 'r', 't', 'h', ' ', 'a', 'n', 'd', ' ', 'e', 'v', 'e', 'r', 'y', 't', 'h', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', '’', 's', ' ', 'i', 'n', ' ', 'i', 't', ',', '\\n', 'a', 'n', 'd', '—', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 's', ' ', 'm', 'o', 'r', 'e', '—', 'y', 'o', 'u', '’', 'l', 'l', ' ', 'b', 'e', ' ', 'a', ' ', 'm', 'a', 'n', ',', ' ', 'm', 'y', ' ', 's', 'o', 'n', '!']\n"
     ]
    }
   ],
   "source": [
    "stem_tokens = [lemmatizer.lemmatize(x, pos=get_wordnet_pos(x)) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the words now we are getting more counts for our bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature-vector creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'i': 25,\n",
       "         'f': 9,\n",
       "         ' ': 60,\n",
       "         'y': 10,\n",
       "         'o': 27,\n",
       "         'u': 16,\n",
       "         'c': 11,\n",
       "         'a': 14,\n",
       "         'n': 27,\n",
       "         't': 24,\n",
       "         'l': 10,\n",
       "         'k': 4,\n",
       "         'w': 8,\n",
       "         'h': 17,\n",
       "         'r': 16,\n",
       "         'd': 7,\n",
       "         's': 14,\n",
       "         'e': 21,\n",
       "         'p': 1,\n",
       "         'v': 4,\n",
       "         ',': 7,\n",
       "         '\\n': 7,\n",
       "         'g': 5,\n",
       "         '—': 3,\n",
       "         'm': 8,\n",
       "         '.': 2,\n",
       "         'b': 2,\n",
       "         'x': 1,\n",
       "         '’': 3,\n",
       "         '!': 1})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bow = Counter(stem_tokens)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the most frequent 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 60),\n",
       " ('o', 27),\n",
       " ('n', 27),\n",
       " ('i', 25),\n",
       " ('t', 24),\n",
       " ('e', 21),\n",
       " ('h', 17),\n",
       " ('u', 16),\n",
       " ('r', 16),\n",
       " ('a', 14)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's remove the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'f': 9,\n",
       "         ' ': 60,\n",
       "         'u': 16,\n",
       "         'c': 11,\n",
       "         'n': 27,\n",
       "         'l': 10,\n",
       "         'k': 4,\n",
       "         'w': 8,\n",
       "         'h': 17,\n",
       "         'r': 16,\n",
       "         'e': 21,\n",
       "         'p': 1,\n",
       "         'v': 4,\n",
       "         ',': 7,\n",
       "         '\\n': 7,\n",
       "         'g': 5,\n",
       "         '—': 3,\n",
       "         '.': 2,\n",
       "         'b': 2,\n",
       "         'x': 1,\n",
       "         '’': 3,\n",
       "         '!': 1})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_tokens = [x for x in stem_tokens if x not in stop_words]\n",
    "count = Counter(no_stop_tokens)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally... let's make our feature vector using the frequency ratio (term count / total number of terms in the doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2553191489361702, 0.1148936170212766, 0.08936170212765958, 0.07234042553191489, 0.06808510638297872, 0.06808510638297872, 0.04680851063829787, 0.0425531914893617, 0.03829787234042553, 0.03404255319148936, 0.029787234042553193, 0.029787234042553193, 0.02127659574468085, 0.01702127659574468, 0.01702127659574468, 0.01276595744680851, 0.01276595744680851, 0.00851063829787234, 0.00851063829787234, 0.00425531914893617, 0.00425531914893617, 0.00425531914893617]\n"
     ]
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(no_stop_tokens)\n",
    "for key, value in count.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "\n",
    "print(document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
