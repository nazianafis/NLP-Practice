{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.6.13 ('elmovis')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n elmovis ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class Elmo:\n",
    "    def __init__(self):\n",
    "        self.elmo = ElmoEmbedder()\n",
    "\n",
    "    def get_elmo_vector(self, tokens, layer):\n",
    "        vectors = self.elmo.embed_sentence(tokens)\n",
    "        X = []\n",
    "        for vector in vectors[layer]:\n",
    "            X.append(vector)\n",
    "\n",
    "        X = np.array(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "def dim_reduction(X, n):\n",
    "    pca = PCA(n_components=n)\n",
    "    print(\"size of X: {}\".format(X.shape))\n",
    "    results = pca.fit_transform(X)\n",
    "    print(\"size of reduced X: {}\".format(results.shape))\n",
    "\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        print(\"Variance retained ratio of PCA-{}: {}\".format(i+1, ratio))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot(word, token_list, reduced_X, file_name, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # plot ELMo vectors\n",
    "    i = 0\n",
    "    for j, token in enumerate(token_list):\n",
    "        color = pick_color(j)\n",
    "        for _, w in enumerate(token):\n",
    "\n",
    "            # only plot the word of interest\n",
    "            if w.lower() in [word, word + 's', word + 'ing', word + 'ed']:\n",
    "                ax.plot(reduced_X[i, 0], reduced_X[i, 1], color)\n",
    "            i += 1\n",
    "\n",
    "    tokens = []\n",
    "    for token in token_list:\n",
    "        tokens += token\n",
    "\n",
    "    # annotate point\n",
    "    k = 0\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.lower() in [word, word + 's', word + 'ing', word + 'ed']:\n",
    "            text = ' '.join(token_list[k])\n",
    "\n",
    "            # bold the word of interest in the sentence\n",
    "            text = text.replace(token, r\"$\\bf{\" + token + \"}$\")\n",
    "\n",
    "            plt.annotate(text, xy=(reduced_X[i, 0], reduced_X[i, 1]))\n",
    "            k += 1\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"PCA 1\")\n",
    "    ax.set_ylabel(\"PCA 2\")\n",
    "    fig.savefig(file_name, bbox_inches=\"tight\")\n",
    "\n",
    "    print(\"{} saved\\n\".format(file_name))\n",
    "\n",
    "\n",
    "def pick_color(i):\n",
    "    if i == 0:\n",
    "        color = 'ro'\n",
    "    elif i == 1:\n",
    "        color = 'bo'\n",
    "    elif i == 2:\n",
    "        color = 'yo'\n",
    "    elif i == 3:\n",
    "        color = 'go'\n",
    "    else:\n",
    "        color = 'co'\n",
    "    return color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visualizing word bank using ELMo layer 1\n",
      "size of X: (37, 1024)\n",
      "size of reduced X: (37, 2)\n",
      "Variance retained ratio of PCA-1: 0.11758369207382202\n",
      "Variance retained ratio of PCA-2: 0.07857831567525864\n",
      "bank_elmo_layer_1.png saved\n",
      "\n",
      "visualizing word work using ELMo layer 1\n",
      "size of X: (35, 1024)\n",
      "size of reduced X: (35, 2)\n",
      "Variance retained ratio of PCA-1: 0.0896001085639\n",
      "Variance retained ratio of PCA-2: 0.08288563042879105\n",
      "work_elmo_layer_1.png saved\n",
      "\n",
      "visualizing word plant using ELMo layer 1\n",
      "size of X: (47, 1024)\n",
      "size of reduced X: (47, 2)\n",
      "Variance retained ratio of PCA-1: 0.09546440839767456\n",
      "Variance retained ratio of PCA-2: 0.07193593680858612\n",
      "plant_elmo_layer_1.png saved\n",
      "\n",
      "visualizing word cricket using ELMo layer 1\n",
      "size of X: (43, 1024)\n",
      "size of reduced X: (43, 2)\n",
      "Variance retained ratio of PCA-1: 0.13385212421417236\n",
      "Variance retained ratio of PCA-2: 0.08338294178247452\n",
      "cricket_elmo_layer_1.png saved\n",
      "\n",
      "visualizing word bank using ELMo layer 2\n",
      "size of X: (37, 1024)\n",
      "size of reduced X: (37, 2)\n",
      "Variance retained ratio of PCA-1: 0.1232706680893898\n",
      "Variance retained ratio of PCA-2: 0.08225090801715851\n",
      "bank_elmo_layer_2.png saved\n",
      "\n",
      "visualizing word work using ELMo layer 2\n",
      "size of X: (35, 1024)\n",
      "size of reduced X: (35, 2)\n",
      "Variance retained ratio of PCA-1: 0.09771403670310974\n",
      "Variance retained ratio of PCA-2: 0.08373767137527466\n",
      "work_elmo_layer_2.png saved\n",
      "\n",
      "visualizing word plant using ELMo layer 2\n",
      "size of X: (47, 1024)\n",
      "size of reduced X: (47, 2)\n",
      "Variance retained ratio of PCA-1: 0.10495452582836151\n",
      "Variance retained ratio of PCA-2: 0.077839195728302\n",
      "plant_elmo_layer_2.png saved\n",
      "\n",
      "visualizing word cricket using ELMo layer 2\n",
      "size of X: (43, 1024)\n",
      "size of reduced X: (43, 2)\n",
      "Variance retained ratio of PCA-1: 0.13131970167160034\n",
      "Variance retained ratio of PCA-2: 0.09377571195363998\n",
      "cricket_elmo_layer_2.png saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Elmo()\n",
    "\n",
    "banks = OrderedDict()\n",
    "banks[0] = \"One can deposit money at the bank\"\n",
    "banks[1] = \"He had a nice walk along the river bank\"\n",
    "banks[2] = \"I withdrew cash from the bank\"\n",
    "banks[3] = \"The river bank was not clean\"\n",
    "banks[4] = \"My wife and I have a joint bank account\"\n",
    "\n",
    "works = OrderedDict()\n",
    "works[0] = \"I like this beautiful work by Andy Warhol\"\n",
    "works[1] = \"Employee works hard every day\"\n",
    "works[2] = \"My sister works at Starbucks\"\n",
    "works[3] = \"This amazing work was done in the early nineteenth century\"\n",
    "works[4] = \"Hundreds of people work in this building\"\n",
    "\n",
    "plants = OrderedDict()\n",
    "plants[0] = \"The gardener planted some trees in my yard\"\n",
    "plants[1] = \"I plan to plant a Joshua tree tomorrow\"\n",
    "plants[2] = \"My sister planted a seed and hopes it will grow to a tree\"\n",
    "plants[3] = \"This kind of plant only grows in the subtropical region\"\n",
    "plants[4] = \"Most of the plants will die without water\"\n",
    "\n",
    "cricket = OrderedDict()\n",
    "cricket[0] = \"The cricket is a very small insect\"\n",
    "cricket[1] = \"The sound produced by the cricket is very high-pitched\"\n",
    "cricket[2] = \"The game of cricket is played in the grass\"\n",
    "cricket[3] = \"The cricket bat used by Sachin Tendulkar is very heavy\"\n",
    "cricket[4] = \"Insects can be as small as the cricket\"\n",
    "words = {\n",
    "    \"bank\": banks,\n",
    "    \"work\": works,\n",
    "    \"plant\": plants,\n",
    "    \"cricket\": cricket\n",
    "}\n",
    "\n",
    "# contextual vectors for ELMo layer 1 and 2\n",
    "for layer in [1, 2]:\n",
    "    for word, sentences in words.items():\n",
    "        print(\"visualizing word {} using ELMo layer {}\".format(word, layer))\n",
    "        X = np.concatenate([model.get_elmo_vector(tokens=sentences[idx].split(),\n",
    "                                                    layer=layer)\n",
    "                            for idx, _ in enumerate(sentences)], axis=0)\n",
    "\n",
    "        # The first 2 principal components\n",
    "        X_reduce = dim_reduction(X=X, n=2)\n",
    "\n",
    "        token_list = []\n",
    "        for _, sentence in sentences.items():\n",
    "            token_list.append(sentence.split())\n",
    "\n",
    "        file_name = \"{}_elmo_layer_{}.png\".format(word, layer)\n",
    "        title = \"Layer {} ELMo vectors of the word {}\".format(layer, word)\n",
    "        plot(word, token_list, X_reduce, file_name, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7553fb56ab976ad5aff909b9ab0e5f0d2a23e397f8c044edf0a757925c0fad4d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
